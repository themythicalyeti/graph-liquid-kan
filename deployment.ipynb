{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Graph-Liquid-KAN Sea Lice Prediction - A100 GPU Training\n## Phase 4: Production Training on Google Colab\n\nThis notebook runs **A100-optimized GPU training** of the **SeaLicePredictor** architecture with **Weights & Biases** integration for experiment tracking.\n\n**Architecture: SeaLicePredictor**\n- **FastKAN Layers**: Gaussian RBF basis functions (learnable non-linearities)\n- **GraphonAggregator**: 1/N normalized message passing (scale invariant)\n- **LiquidKANCell**: Closed-form Continuous (CfC) dynamics with adaptive tau\n- **BelehradekKAN**: Temperature-dependent development rate\n- **SalinityMortalityKAN**: Salinity survival factor\n- **LarvalTransportModule**: Cross-farm infection via ocean currents\n- **Physics-Informed Loss**: Tweedie (p=1.5) + L_bio\n\n**Key Features:**\n1. **Tweedie Loss (p=1.5)**: Proper loss for zero-inflated count data. Prevents \"mean reversion\" where model predicts dataset mean for all inputs.\n2. **Dynamic Learning Rate**: Starts at lr=1e-2, reduces by 0.5x after 15 epochs without improvement (ReduceLROnPlateau)\n3. **Conformal Prediction**: 90% coverage prediction intervals for uncertainty quantification\n4. **Risk-Aware Detection**: Uses upper bounds for conservative outbreak detection\n\n**Outbreak Threshold:** 0.5 adult female lice per fish (Norwegian regulatory threshold)\n\n**Target Metrics:**\n| Metric | Target | Description |\n|--------|--------|-------------|\n| Recall | >=90% | Catch 9/10 outbreaks |\n| Precision | >=80% | 8/10 predictions correct |\n| F1 Score | >=0.85 | Balance P/R |\n| Conformal Coverage | 90% | Prediction intervals contain true value |\n\n**Runtime Configuration:**\n- Runtime -> Change runtime type -> **A100 GPU**\n- Runtime -> Change runtime type -> **High RAM**\n\n**Setup:**\n1. Get wandb API key from https://wandb.ai/authorize\n2. Upload `glkan_data.zip` to Google Drive root (must contain tensors.npz with feature_indices)\n3. Run all cells"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/GLKAN_Project/checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/GLKAN_Project/outputs', exist_ok=True)\n",
    "print('Google Drive mounted and directories created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone the Graph-Liquid-KAN repository\n",
    "REPO_URL = 'https://github.com/themythicalyeti/graph-liquid-kan.git'\n",
    "REPO_DIR = '/content/graph-liquid-kan'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f'Repository already exists at {REPO_DIR}')\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "print(f'\\nWorking directory: {os.getcwd()}')\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch torchvision --upgrade\n!pip install -q numpy pandas scipy scikit-learn\n!pip install -q loguru tqdm matplotlib\n!pip install -q torch-geometric\n!pip install -q wandb\n\nprint('\\nDependencies installed')\n\n# =============================================================================\n# WANDB AUTHENTICATION\n# =============================================================================\n# Option 1: Use Colab Secrets (recommended - no prompt each time)\n#   1. Click the key icon in left sidebar\n#   2. Add secret named \"WANDB_API_KEY\" with your key from https://wandb.ai/authorize\n#\n# Option 2: Manual login (will prompt for API key)\n#   Just run the cell - it will ask for your key\n\nimport wandb\nimport os\n\ntry:\n    from google.colab import userdata\n    WANDB_KEY = userdata.get('WANDB_API_KEY')\n    os.environ['WANDB_API_KEY'] = WANDB_KEY\n    wandb.login(key=WANDB_KEY)\n    print('Logged in to wandb using Colab Secrets')\nexcept:\n    print('Colab Secrets not configured - will prompt for API key')\n    print('Get your key at: https://wandb.ai/authorize')\n    wandb.login()\n    \nprint(f'wandb authenticated as: {wandb.api.viewer()[\"entity\"]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!nvidia-smi\n\nimport torch\nimport gc\n\nprint('\\n' + '='*60)\nprint('GPU VERIFICATION')\nprint('='*60)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"VRAM: {gpu_memory:.1f} GB\")\n    device = torch.device('cuda')\n    \n    # Enable TF32 for faster training on Ampere GPUs\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # IMPORTANT: Disable AMP - sparse matrix ops don't support FP16\n    # torch.sparse.mm raises NotImplementedError for 'Half' dtype\n    USE_AMP = False\n    print(f\"Mixed Precision (AMP): Disabled (sparse ops don't support FP16)\")\n    \n    torch.cuda.empty_cache()\n    gc.collect()\nelse:\n    print(\"[WARN] CUDA not available - using CPU (will be VERY slow)\")\n    print(\"[WARN] Please enable GPU: Runtime -> Change runtime type -> T4 GPU\")\n    device = torch.device('cpu')\n    USE_AMP = False\n\nprint(f\"\\nDefault device: {device}\")\nprint('='*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Data from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Path to data on Drive\n",
    "DRIVE_DATA = '/content/drive/MyDrive/glkan_data.zip'\n",
    "LOCAL_DATA = '/content/data'\n",
    "\n",
    "if not os.path.exists(DRIVE_DATA):\n",
    "    print(f'ERROR: Data not found at {DRIVE_DATA}')\n",
    "    print('Please upload glkan_data.zip containing:')\n",
    "    print('  - tensors.npz (from Phase 2)')\n",
    "    print('  - spatial_graph.pt (from Phase 2)')\n",
    "else:\n",
    "    print('Extracting data...')\n",
    "    os.makedirs(LOCAL_DATA, exist_ok=True)\n",
    "    !unzip -q \"{DRIVE_DATA}\" -d {LOCAL_DATA}\n",
    "    !ls -la {LOCAL_DATA}\n",
    "    print('\\nData loaded')\n",
    "\n",
    "# Verify data\n",
    "TENSOR_PATH = f'{LOCAL_DATA}/tensors.npz'\n",
    "GRAPH_PATH = f'{LOCAL_DATA}/spatial_graph.pt'\n",
    "\n",
    "if os.path.exists(TENSOR_PATH) and os.path.exists(GRAPH_PATH):\n",
    "    data = np.load(TENSOR_PATH, allow_pickle=True)\n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"  X (features): {data['X'].shape}\")\n",
    "    print(f\"  Y (targets):  {data['Y'].shape}\")\n",
    "    print(f\"  mask:         {data['mask'].shape}\")\n",
    "    \n",
    "    graph = torch.load(GRAPH_PATH, weights_only=False)\n",
    "    print(f\"  edges:        {graph['edge_index'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Import GLKAN Architecture from Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '/content/graph-liquid-kan')\n\n# Import architecture from src/models\nfrom src.models import (\n    FastKAN,\n    GraphonAggregator,\n    LiquidKANCell,\n    GraphLiquidKANCell,\n    GLKANNetwork,\n    GLKANPredictor,\n)\n\n# Import SeaLicePredictor (domain-specific with biological modules)\nfrom src.models.sea_lice_network import SeaLicePredictor\n\n# Import conformal prediction for uncertainty quantification\nfrom src.models.conformal import ConformalSeaLicePredictor\n\n# Import training utilities from src/training\nfrom src.training import PhysicsInformedLoss, GLKANLoss\nfrom src.training.losses import LossConfig\nfrom src.training.trainer import TrainingConfig\n\n# Import dataset from src/data\nfrom src.data import SeaLiceGraphDataset\n\nprint('Imported from repository:')\nprint('  - FastKAN, GraphonAggregator, LiquidKANCell')\nprint('  - GraphLiquidKANCell, GLKANNetwork, GLKANPredictor')\nprint('  - SeaLicePredictor (domain-specific with biology modules)')\nprint('  - ConformalSeaLicePredictor (uncertainty quantification)')\nprint('  - PhysicsInformedLoss, GLKANLoss, LossConfig, TrainingConfig')\nprint('  - SeaLiceGraphDataset')\nprint('\\nGraph-Liquid-KAN architecture loaded from src/')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import Dataset, DataLoader\n\nclass SeaLiceDataset(Dataset):\n    \"\"\"Dataset for GLKAN training with feature_indices support.\"\"\"\n    \n    def __init__(self, X, Y, mask, edge_index, feature_indices=None, window_size=30, stride=7, time_start=0, time_end=None):\n        self.X = X\n        self.Y = Y\n        self.mask = mask\n        self.edge_index = edge_index\n        self.feature_indices = feature_indices\n        self.window_size = window_size\n        \n        time_end = time_end or X.shape[0]\n        self.sequences = []\n        for t in range(time_start, time_end - window_size, stride):\n            self.sequences.append((t, t + window_size))\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        t_start, t_end = self.sequences[idx]\n        return {\n            'x': self.X[t_start:t_end],\n            'y': self.Y[t_start:t_end],\n            'mask': self.mask[t_start:t_end],\n            'edge_index': self.edge_index,\n            'feature_indices': self.feature_indices,\n        }\n\ndef collate_fn(batch):\n    return {\n        'x': torch.stack([b['x'] for b in batch]),\n        'y': torch.stack([b['y'] for b in batch]),\n        'mask': torch.stack([b['mask'] for b in batch]),\n        'edge_index': batch[0]['edge_index'],\n        'feature_indices': batch[0]['feature_indices'],\n    }\n\n# Load data\ndata = np.load(TENSOR_PATH, allow_pickle=True)\ngraph = torch.load(GRAPH_PATH, weights_only=False)\n\nX = torch.from_numpy(data['X']).float()\nY = torch.from_numpy(data['Y']).float()\nmask = torch.from_numpy(data['mask']).bool()\nedge_index = graph['edge_index']\n\n# Load feature_indices for SeaLicePredictor biological modules\nif 'feature_indices' in data:\n    feature_indices = data['feature_indices'].item()\n    print(f'Loaded feature_indices: {list(feature_indices.keys())}')\nelse:\n    feature_indices = None\n    print('WARNING: No feature_indices found - biological modules will use defaults')\n\nprint(f'\\nData loaded:')\nprint(f'  X: {X.shape}')\nprint(f'  Y: {Y.shape}')\nprint(f'  edge_index: {edge_index.shape}')\n\n# Train/Val/Test split (70/15/15)\nT_total = X.shape[0]\nT_train = int(T_total * 0.70)\nT_val = int(T_total * 0.85)\n\n# Configuration\nWINDOW_SIZE = 30\nSTRIDE = 7\nBATCH_SIZE = 8\n\ntrain_ds = SeaLiceDataset(X, Y, mask, edge_index, feature_indices, WINDOW_SIZE, STRIDE, 0, T_train)\nval_ds = SeaLiceDataset(X, Y, mask, edge_index, feature_indices, WINDOW_SIZE, STRIDE, T_train, T_val)\ntest_ds = SeaLiceDataset(X, Y, mask, edge_index, feature_indices, WINDOW_SIZE, STRIDE, T_val, T_total)\n\n# NOTE: num_workers=0 required for Google Colab (multiprocessing causes crashes)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=collate_fn, num_workers=0, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n                        collate_fn=collate_fn, num_workers=0, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n                         collate_fn=collate_fn, num_workers=0, pin_memory=True)\n\nprint(f'\\nDataLoaders created:')\nprint(f'  Train sequences: {len(train_ds)}')\nprint(f'  Val sequences: {len(val_ds)}')\nprint(f'  Test sequences: {len(test_ds)}')\nprint(f'  Nodes: {X.shape[1]}')\nprint(f'  Edges: {edge_index.shape[1]}')"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 6b: Pre-training with Physics-Informed Outbreak Augmentation (Optional)\n\n**Why Pre-train?**\nReal sea lice data is heavily imbalanced:\n- ~95% of observations are low/normal lice levels\n- ~5% are outbreaks (above 0.5 adult female lice)\n\nThe model rarely sees outbreaks during training, making it hard to learn outbreak patterns.\n\n**Solution: HybridSpatialOutbreakSimulator**\n- Generates synthetic outbreak scenarios using physics-informed simulation\n- Uses **dynamic flux** from ocean currents (larvae follow water flow)\n- Supports counterfactual \"nightmare\" scenarios (warmer + stronger currents)\n- Creates balanced pre-training dataset (50% outbreaks)\n\n**Workflow:**\n1. Train simulator on real data (learns epidemic dynamics)\n2. Generate synthetic data with controlled outbreak ratio\n3. Pre-train model on synthetic data\n4. Fine-tune on real data (next cells)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# PHYSICS-INFORMED OUTBREAK AUGMENTATION\n# =============================================================================\n# Set to True to enable pre-training with synthetic outbreak data\nENABLE_PRETRAINING = True\n\n# Pre-training configuration\nPRETRAIN_CONFIG = {\n    'simulator_epochs': 50,      # Epochs to train the outbreak simulator\n    'simulator_lr': 1e-3,        # Learning rate for simulator\n    'n_synthetic_samples': 500,  # Number of synthetic sequences to generate\n    'outbreak_ratio': 0.5,       # Fraction of outbreaks (vs ~5% in real data)\n    'pretrain_epochs': 20,       # Epochs of pre-training on synthetic data\n    'pretrain_lr': 1e-3,         # Lower LR for pre-training\n    # Nightmare scenario parameters\n    'temperature_range': (3.0, 16.0),  # Temperature perturbation range (°C) - realistic Norwegian coastal temps\n    'current_range': (0.8, 1.5),        # Current scaling range\n}\n\nif ENABLE_PRETRAINING:\n    print('='*60)\n    print('PHYSICS-INFORMED OUTBREAK AUGMENTATION')\n    print('='*60)\n    \n    # Check if graph has required static edge features\n    if 'edge_direction' not in graph or 'edge_distance' not in graph:\n        print('\\n[WARN] Graph missing edge_direction/edge_distance')\n        print('       Run add_static_edge_features_to_graph() first')\n        print('       Skipping pre-training...')\n        ENABLE_PRETRAINING = False\n    else:\n        print(f'\\nGraph has required static edge features:')\n        print(f'  edge_direction: {graph[\"edge_direction\"].shape}')\n        print(f'  edge_distance: {graph[\"edge_distance\"].shape}')\n\nif ENABLE_PRETRAINING:\n    from src.models.timegan import HybridSpatialOutbreakSimulator, HybridOutbreakAugmenter\n    from tqdm.auto import tqdm\n    import torch.nn.functional as F\n    \n    # Load feature_indices from JSON if needed\n    if feature_indices is None:\n        import json\n        if 'feature_indices_json' in data.files:\n            fi_json = data['feature_indices_json'].item()\n            feature_indices = json.loads(fi_json)\n            print(f'\\nLoaded feature_indices: {list(feature_indices.keys())}')\n    \n    # ==========================================================================\n    # Step 1: Create HybridSpatialOutbreakSimulator\n    # ==========================================================================\n    print('\\n--- Step 1: Creating Hybrid Outbreak Simulator ---')\n    \n    simulator = HybridSpatialOutbreakSimulator(\n        n_farms=graph['n_nodes'],\n        edge_index=graph['edge_index'],\n        edge_distance=graph['edge_distance'],\n        edge_direction=graph['edge_direction'],\n        feature_indices=feature_indices,\n        feature_dim=Y.shape[-1],  # 3: adult female, mobile, attached\n        hidden_dim=32,            # Smaller than main model\n        decay_km=15.0,            # Larval survival distance\n    ).to(device)\n    \n    n_sim_params = sum(p.numel() for p in simulator.parameters())\n    print(f'  Simulator parameters: {n_sim_params:,}')\n    print(f'  Device: {device}')\n    \n    # ==========================================================================\n    # Step 2: Train simulator on real outbreak patterns\n    # ==========================================================================\n    print('\\n--- Step 2: Training Simulator on Real Data ---')\n    \n    # Use training data portion\n    X_train = X[:T_train].to(device)\n    Y_train = Y[:T_train].to(device)\n    mask_train = mask[:T_train].to(device)\n    \n    sim_optimizer = torch.optim.Adam(simulator.parameters(), lr=PRETRAIN_CONFIG['simulator_lr'])\n    \n    print(f'  Training on {T_train} timesteps...')\n    \n    for epoch in tqdm(range(PRETRAIN_CONFIG['simulator_epochs']), desc='Training simulator'):\n        simulator.train()\n        sim_optimizer.zero_grad()\n        \n        # Forward pass (flux computed dynamically!)\n        Y_pred = simulator.forward(X_train, initial_lice=Y_train[0], add_noise=False)\n        \n        # MSE loss on valid observations\n        diff = (Y_pred - Y_train) ** 2\n        loss = (diff * mask_train.unsqueeze(-1).float()).sum() / mask_train.float().sum().clamp(min=1)\n        \n        loss.backward()\n        sim_optimizer.step()\n        \n        if (epoch + 1) % 10 == 0:\n            print(f'  Epoch {epoch+1}: loss={loss.item():.4f}')\n    \n    simulator.eval()\n    print(f'  Final simulator loss: {loss.item():.4f}')\n    \n    # ==========================================================================\n    # Step 3: Generate synthetic outbreak scenarios\n    # ==========================================================================\n    print('\\n--- Step 3: Generating Synthetic Outbreak Data ---')\n    \n    n_samples = PRETRAIN_CONFIG['n_synthetic_samples']\n    n_outbreaks = int(n_samples * PRETRAIN_CONFIG['outbreak_ratio'])\n    n_normal = n_samples - n_outbreaks\n    \n    print(f'  Generating {n_normal} normal + {n_outbreaks} outbreak scenarios...')\n    print(f'  Using dynamic flux from ocean currents')\n    \n    synthetic_X = []  # Node features (for context)\n    synthetic_Y = []  # Lice counts (targets)\n    \n    # Use a sample of real X for environmental context\n    X_sample = X[:WINDOW_SIZE].to(device)\n    \n    # Generate outbreak scenarios (perturbed conditions)\n    for i in tqdm(range(n_outbreaks), desc='Generating outbreaks'):\n        # Random perturbation\n        temp_perturb = PRETRAIN_CONFIG['temperature_range'][0] + \\\n                       torch.rand(1).item() * (PRETRAIN_CONFIG['temperature_range'][1] - PRETRAIN_CONFIG['temperature_range'][0])\n        current_scale = PRETRAIN_CONFIG['current_range'][0] + \\\n                        torch.rand(1).item() * (PRETRAIN_CONFIG['current_range'][1] - PRETRAIN_CONFIG['current_range'][0])\n        \n        with torch.no_grad():\n            scenario = simulator.generate_outbreak_scenarios(\n                n_scenarios=1,\n                X=X_sample,\n                initial_intensity=0.3,\n                temperature_perturbation=temp_perturb,\n                current_scale=current_scale,\n                device=device,\n            )\n        synthetic_Y.append(scenario[0].cpu())\n        synthetic_X.append(X_sample.cpu())\n    \n    # Generate normal scenarios\n    for i in tqdm(range(n_normal), desc='Generating normal'):\n        with torch.no_grad():\n            initial = torch.zeros(X.shape[1], Y.shape[-1], device=device)\n            initial[:, 0] = torch.rand(X.shape[1], device=device) * 0.1  # Small random initial\n            \n            scenario = simulator.forward(X_sample, initial_lice=initial, add_noise=True)\n        synthetic_Y.append(scenario.cpu())\n        synthetic_X.append(X_sample.cpu())\n    \n    # Stack into tensors\n    synthetic_X = torch.stack(synthetic_X, dim=0)  # (n_samples, T, N, F)\n    synthetic_Y = torch.stack(synthetic_Y, dim=0)  # (n_samples, T, N, 3)\n    synthetic_mask = torch.ones(n_samples, WINDOW_SIZE, X.shape[1], dtype=torch.bool)\n    \n    print(f'\\nSynthetic data generated:')\n    print(f'  X shape: {synthetic_X.shape}')\n    print(f'  Y shape: {synthetic_Y.shape}')\n    print(f'  Y range: [{synthetic_Y.min():.4f}, {synthetic_Y.max():.4f}]')\n    \n    # Check outbreak rate\n    outbreak_threshold = 0.5\n    has_outbreak = (synthetic_Y[:, :, :, 0] > outbreak_threshold).any(dim=(1, 2))\n    actual_outbreak_rate = has_outbreak.float().mean().item()\n    print(f'  Actual outbreak rate: {actual_outbreak_rate:.1%}')\n    \n    # ==========================================================================\n    # Step 4: Create pre-training DataLoader\n    # ==========================================================================\n    print('\\n--- Step 4: Creating Pre-training DataLoader ---')\n    \n    class SyntheticDataset(Dataset):\n        def __init__(self, X, Y, mask, edge_index, feature_indices):\n            self.X = X\n            self.Y = Y\n            self.mask = mask\n            self.edge_index = edge_index\n            self.feature_indices = feature_indices\n        \n        def __len__(self):\n            return len(self.X)\n        \n        def __getitem__(self, idx):\n            return {\n                'x': self.X[idx],\n                'y': self.Y[idx],\n                'mask': self.mask[idx],\n                'edge_index': self.edge_index,\n                'feature_indices': self.feature_indices,\n            }\n    \n    pretrain_ds = SyntheticDataset(synthetic_X, synthetic_Y, synthetic_mask, edge_index, feature_indices)\n    pretrain_loader = DataLoader(pretrain_ds, batch_size=BATCH_SIZE, shuffle=True,\n                                  collate_fn=collate_fn, num_workers=0, pin_memory=True)\n    \n    print(f'  Pre-training batches: {len(pretrain_loader)}')\n    \n    # Clear simulator from GPU memory\n    del simulator, sim_optimizer, X_train, Y_train, mask_train\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print('\\n' + '='*60)\n    print('SYNTHETIC DATA READY FOR PRE-TRAINING')\n    print('='*60)\n    print(f'  Samples: {n_samples}')\n    print(f'  Outbreak ratio: {actual_outbreak_rate:.1%}')\n    print(f'  Window size: {WINDOW_SIZE} days')\n    print('\\nPre-training will run in the main training loop (Cell 8)')\nelse:\n    print('\\n[INFO] Pre-training disabled (ENABLE_PRETRAINING = False)')\n    pretrain_loader = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Create Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration - tuned for 1777 nodes on A100 (80GB)\n# Using SeaLicePredictor with biological modules and dynamic LR\nCONFIG = {\n    'hidden_dim': 64,      # Reduced from 128 (OOM fix)\n    'n_bases': 8,          # Reduced from 12\n    'k_hops': 3,           # Multi-hop spatial aggregation\n    'dropout': 0.1,\n    'lr': 1e-2,            # CRITICAL: Must be >= 1e-2 for Tweedie loss to work!\n    'weight_decay': 1e-4,  # Prevents spline coefficient oscillation\n    'grad_clip': 1.0,\n    'epochs': 100,\n    'lambda_bio': 0.01,    # Bio penalty\n    'lambda_stability': 0.001,\n    # Dynamic LR scheduler (ReduceLROnPlateau)\n    'scheduler_patience': 15,  # Epochs without improvement before reducing LR\n    'scheduler_factor': 0.5,   # Multiply LR by this when reducing\n    'min_lr': 1e-6,            # Don't reduce below this\n    # Early stopping DISABLED - will run all epochs\n    'early_stopping_patience': 10000,  # Set very high to effectively disable\n    'min_delta': 1e-6,\n    # Data config\n    'n_nodes': X.shape[1],\n    'n_edges': edge_index.shape[1],\n    'n_features': X.shape[-1],\n    'window_size': WINDOW_SIZE,\n    'stride': STRIDE,\n    'batch_size': 4,       # Reduced from 16 (OOM fix)\n    # ==========================================================================\n    # INPUT DENORMALIZATION FOR BIOLOGICAL MODULES\n    # ==========================================================================\n    # These values are used to convert z-scored temperature/salinity back to\n    # raw physical units (°C, PSU) before passing to BelehradekKAN and\n    # SalinityMortalityKAN. Without these, the biological modules receive\n    # normalized values and output constants.\n    'temp_mean': 9.30,     # Mean temperature in °C (from data normalization)\n    'temp_std': 3.94,      # Temperature std in °C\n    'sal_mean': 31.96,     # Mean salinity in PSU\n    'sal_std': 2.84,       # Salinity std in PSU\n}\n\nBATCH_SIZE = CONFIG['batch_size']\n\n# Initialize wandb run (already authenticated in Cell 2)\nprint('='*60)\nprint('INITIALIZING WANDB RUN')\nprint('='*60)\n\nrun = wandb.init(\n    project=\"graph-liquid-kan\",\n    name=f\"sealice-predictor-lr{CONFIG['lr']}-{CONFIG['hidden_dim']}h\",\n    config=CONFIG,\n    tags=[\"A100\", \"sea-lice\", \"tweedie-loss\", \"SeaLicePredictor\", \"dynamic-lr\"],\n)\n\nif torch.cuda.is_available():\n    wandb.config.update({\n        'gpu_name': torch.cuda.get_device_name(0),\n        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9,\n        'mixed_precision': USE_AMP,\n    })\n\nprint(f'wandb run: {wandb.run.name}')\nprint(f'wandb URL: {wandb.run.get_url()}')\n\n# Keep edge_index on CPU in datasets (required for num_workers > 0)\n# We'll move it to GPU once before training and use that reference\nprint('\\n' + '='*60)\nprint('SETTING UP DATA PIPELINE')\nprint('='*60)\n\n# Datasets keep CPU edge_index (for multiprocessing compatibility)\n# NOTE: num_workers=0 required for Google Colab (multiprocessing causes crashes)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=collate_fn, num_workers=0, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n                        collate_fn=collate_fn, num_workers=0, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n                         collate_fn=collate_fn, num_workers=0, pin_memory=True)\n\n# Create a GPU copy of edge_index to use in training loop\n# This avoids moving it every batch (it's constant)\nedge_index_gpu = edge_index.to(device)\nprint(f'  edge_index_gpu created on: {edge_index_gpu.device}')\nprint(f'  DataLoaders using num_workers=0 (Colab compatibility)')\nprint(f'  Batch size: {BATCH_SIZE}')\n\n# Create SeaLicePredictor model (domain-specific with biological modules)\ninput_dim = X.shape[-1]\noutput_dim = Y.shape[-1]\n\n# =============================================================================\n# MODEL CREATION WITH INPUT DENORMALIZATION\n# =============================================================================\n# The temp_mean/std and sal_mean/std parameters enable biological modules\n# (BelehradekKAN, SalinityMortalityKAN) to receive raw physical values\n# instead of z-scored values. This is CRITICAL for proper biological response.\nmodel = SeaLicePredictor(\n    input_dim=input_dim,\n    hidden_dim=CONFIG['hidden_dim'],\n    output_dim=output_dim,\n    n_bases=CONFIG['n_bases'],\n    k_hops=CONFIG['k_hops'],\n    dropout=CONFIG['dropout'],\n    # Input denormalization for biological modules\n    temp_mean=CONFIG['temp_mean'],\n    temp_std=CONFIG['temp_std'],\n    sal_mean=CONFIG['sal_mean'],\n    sal_std=CONFIG['sal_std'],\n).to(device)\n\n# Optimizer - AdamW with weight decay\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CONFIG['lr'],\n    weight_decay=CONFIG['weight_decay'],\n)\n\n# Scheduler - ReduceLROnPlateau (reduces LR when val_loss plateaus)\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nscheduler = ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=CONFIG['scheduler_factor'],\n    patience=CONFIG['scheduler_patience'],\n    min_lr=CONFIG['min_lr'],\n)\n\n# Create loss function with LossConfig\n# IMPORTANT: Uses Tweedie loss (p=1.5) for zero-inflated count data\nloss_config = LossConfig(\n    loss_type='tweedie',\n    tweedie_p=1.5,\n    lambda_bio=CONFIG['lambda_bio'],\n    lambda_stability=CONFIG['lambda_stability'],\n)\ncriterion = PhysicsInformedLoss(config=loss_config)\n\nprint(f'\\nModel: SeaLicePredictor (with biological modules)')\nprint(f'  - BelehradekKAN: Temperature-dependent development')\nprint(f'  - SalinityMortalityKAN: Salinity survival factor')\nprint(f'  - LarvalTransportModule: Cross-farm infection')\nprint(f'\\nInput Denormalization (for biological modules):')\nprint(f'  - Temperature: mean={CONFIG[\"temp_mean\"]}°C, std={CONFIG[\"temp_std\"]}°C')\nprint(f'  - Salinity: mean={CONFIG[\"sal_mean\"]} PSU, std={CONFIG[\"sal_std\"]} PSU')\nprint(f'\\nLoss function: Tweedie (p={loss_config.tweedie_p})')\nprint(f'  - Handles zero-inflated count data (many farms have 0 lice)')\nprint(f'  - Prevents convergence to dataset mean')\nprint(f'\\nLearning Rate Schedule:')\nprint(f'  - Initial: {CONFIG[\"lr\"]}')\nprint(f'  - Reduces by {CONFIG[\"scheduler_factor\"]}x after {CONFIG[\"scheduler_patience\"]} epochs w/o improvement')\nprint(f'  - Min LR: {CONFIG[\"min_lr\"]}')\nprint(f'\\nEarly Stopping: DISABLED (will run all {CONFIG[\"epochs\"]} epochs)')\n\nscaler = torch.cuda.amp.GradScaler() if USE_AMP else None\n\nn_params = sum(p.numel() for p in model.parameters())\nwandb.config.update({\n    'n_parameters': n_params, \n    'loss_type': 'tweedie', \n    'tweedie_p': 1.5,\n    'model_type': 'SeaLicePredictor',\n})\nwandb.watch(model, log='all', log_freq=100)\n\n# Clear cache before training\ntorch.cuda.empty_cache()\ngc.collect()\n\nprint(f'\\nModel created:')\nprint(f'  Parameters: {n_params:,}')\nprint(f'  Device: {next(model.parameters()).device}')\nprint(f'  Hidden dim: {CONFIG[\"hidden_dim\"]}')\nprint(f'  K-hops: {CONFIG[\"k_hops\"]}')\nprint(f'  Batch size: {BATCH_SIZE}')\n\n# Show memory usage\nif torch.cuda.is_available():\n    mem_alloc = torch.cuda.memory_allocated() / 1e9\n    mem_reserved = torch.cuda.memory_reserved() / 1e9\n    print(f'\\nGPU Memory:')\n    print(f'  Allocated: {mem_alloc:.2f} GB')\n    print(f'  Reserved:  {mem_reserved:.2f} GB')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 8: Training Loop (with Pre-training)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tqdm.auto import tqdm\nimport time\n\nEPOCHS = CONFIG['epochs']\nPATIENCE = CONFIG['early_stopping_patience']\nCHECKPOINT_DIR = '/content/drive/MyDrive/GLKAN_Project/checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# =============================================================================\n# PRE-TRAINING PHASE (if enabled)\n# =============================================================================\nif ENABLE_PRETRAINING and pretrain_loader is not None:\n    print('='*60)\n    print('PRE-TRAINING ON SYNTHETIC OUTBREAK DATA')\n    print('='*60)\n    \n    pretrain_epochs = PRETRAIN_CONFIG['pretrain_epochs']\n    pretrain_lr = PRETRAIN_CONFIG['pretrain_lr']\n    \n    # Use separate optimizer with lower LR for pre-training\n    pretrain_optimizer = torch.optim.AdamW(\n        model.parameters(),\n        lr=pretrain_lr,\n        weight_decay=CONFIG['weight_decay'],\n    )\n    \n    print(f'Pre-training epochs: {pretrain_epochs}')\n    print(f'Pre-training LR: {pretrain_lr}')\n    print(f'Synthetic samples: {len(pretrain_loader.dataset)}')\n    \n    pretrain_history = []\n    \n    for epoch in range(pretrain_epochs):\n        model.train()\n        epoch_loss = 0\n        n_batches = 0\n        \n        pbar = tqdm(pretrain_loader, desc=f'Pre-train {epoch+1}/{pretrain_epochs}', leave=False)\n        for batch in pbar:\n            batch = {\n                'x': batch['x'].to(device, non_blocking=True),\n                'y': batch['y'].to(device, non_blocking=True),\n                'mask': batch['mask'].to(device, non_blocking=True),\n                'edge_index': edge_index_gpu,\n                'feature_indices': batch['feature_indices'],\n            }\n            \n            pretrain_optimizer.zero_grad(set_to_none=True)\n            \n            output = model(batch)\n            loss, _ = criterion(output['predictions'], batch['y'], batch['mask'])\n            \n            if torch.isnan(loss):\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n            pretrain_optimizer.step()\n            \n            epoch_loss += loss.item()\n            n_batches += 1\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        avg_loss = epoch_loss / max(n_batches, 1)\n        pretrain_history.append(avg_loss)\n        print(f'Pre-train Epoch {epoch+1}/{pretrain_epochs} | Loss: {avg_loss:.4f}')\n        \n        wandb.log({\n            'pretrain/epoch': epoch + 1,\n            'pretrain/loss': avg_loss,\n        })\n    \n    print(f'\\nPre-training complete!')\n    print(f'Final pre-train loss: {pretrain_history[-1]:.4f}')\n    \n    # Log pre-training to wandb\n    wandb.config.update({\n        'pretrain_enabled': True,\n        'pretrain_epochs': pretrain_epochs,\n        'pretrain_lr': pretrain_lr,\n        'pretrain_samples': len(pretrain_loader.dataset),\n        'pretrain_outbreak_ratio': PRETRAIN_CONFIG['outbreak_ratio'],\n    })\n    \n    # Clear pre-training data from memory\n    del pretrain_loader, pretrain_optimizer\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    print('\\n' + '='*60)\n    print('STARTING FINE-TUNING ON REAL DATA')\n    print('='*60)\nelse:\n    print('\\n[INFO] Skipping pre-training (disabled or no data)')\n    wandb.config.update({'pretrain_enabled': False})\n\n# =============================================================================\n# MAIN TRAINING LOOP (Fine-tuning on real data)\n# =============================================================================\nbest_val_loss = float('inf')\npatience_counter = 0\nhistory = {'train_loss': [], 'val_loss': [], 'train_rmse': [], 'val_rmse': [], 'lr': []}\n\nprint('='*60)\nprint('GRAPH-LIQUID-KAN TRAINING (SeaLicePredictor)')\nprint('='*60)\nprint(f'Device: {device}')\nprint(f'Epochs: {EPOCHS}')\nprint(f'Initial LR: {CONFIG[\"lr\"]} (reduces by {CONFIG[\"scheduler_factor\"]}x after {CONFIG[\"scheduler_patience\"]} epochs w/o improvement)')\nprint(f'wandb: {wandb.run.get_url()}')\n\nstart_time = time.time()\n\nfor epoch in range(EPOCHS):\n    epoch_start = time.time()\n    \n    # Training\n    model.train()\n    train_loss = 0\n    train_rmse = 0\n    n_batches = 0\n    \n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}', leave=False)\n    for batch_idx, batch in enumerate(pbar):\n        batch = {\n            'x': batch['x'].to(device, non_blocking=True),\n            'y': batch['y'].to(device, non_blocking=True),\n            'mask': batch['mask'].to(device, non_blocking=True),\n            'edge_index': edge_index_gpu,\n            'feature_indices': batch['feature_indices'],\n        }\n        \n        optimizer.zero_grad(set_to_none=True)\n        \n        if USE_AMP:\n            with torch.cuda.amp.autocast():\n                output = model(batch)\n                loss, metrics = criterion(output['predictions'], batch['y'], batch['mask'])\n            \n            if torch.isnan(loss):\n                continue\n            \n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            output = model(batch)\n            loss, metrics = criterion(output['predictions'], batch['y'], batch['mask'])\n            \n            if torch.isnan(loss):\n                continue\n            \n            loss.backward()\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n            optimizer.step()\n        \n        train_loss += loss.item()\n        \n        with torch.no_grad():\n            mask_exp = batch['mask'].unsqueeze(-1).expand_as(output['predictions'])\n            sq_err = ((output['predictions'] - batch['y']) ** 2) * mask_exp.float()\n            rmse = torch.sqrt(sq_err.sum() / mask_exp.float().sum().clamp(min=1))\n            train_rmse += rmse.item()\n        \n        n_batches += 1\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        if batch_idx % 10 == 0:\n            wandb.log({'batch/loss': loss.item(), 'batch/rmse': rmse.item()}, commit=False)\n    \n    train_loss /= max(n_batches, 1)\n    train_rmse /= max(n_batches, 1)\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    val_rmse = 0\n    n_val = 0\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {\n                'x': batch['x'].to(device, non_blocking=True),\n                'y': batch['y'].to(device, non_blocking=True),\n                'mask': batch['mask'].to(device, non_blocking=True),\n                'edge_index': edge_index_gpu,\n                'feature_indices': batch['feature_indices'],\n            }\n            \n            if USE_AMP:\n                with torch.cuda.amp.autocast():\n                    output = model(batch)\n                    loss, _ = criterion(output['predictions'], batch['y'], batch['mask'])\n            else:\n                output = model(batch)\n                loss, _ = criterion(output['predictions'], batch['y'], batch['mask'])\n            \n            val_loss += loss.item()\n            \n            mask_exp = batch['mask'].unsqueeze(-1).expand_as(output['predictions'])\n            sq_err = ((output['predictions'] - batch['y']) ** 2) * mask_exp.float()\n            rmse = torch.sqrt(sq_err.sum() / mask_exp.float().sum().clamp(min=1))\n            val_rmse += rmse.item()\n            n_val += 1\n    \n    val_loss /= max(n_val, 1)\n    val_rmse /= max(n_val, 1)\n    \n    # Update scheduler (ReduceLROnPlateau) - based on val_loss\n    old_lr = optimizer.param_groups[0]['lr']\n    scheduler.step(val_loss)\n    new_lr = optimizer.param_groups[0]['lr']\n    \n    if new_lr < old_lr:\n        print(f'>>> LR REDUCED: {old_lr:.2e} -> {new_lr:.2e} <<<')\n    \n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['train_rmse'].append(train_rmse)\n    history['val_rmse'].append(val_rmse)\n    history['lr'].append(new_lr)\n    \n    epoch_time = time.time() - epoch_start\n    \n    # Log to wandb\n    wandb.log({\n        'epoch': epoch + 1,\n        'train/loss': train_loss,\n        'train/rmse': train_rmse,\n        'val/loss': val_loss,\n        'val/rmse': val_rmse,\n        'learning_rate': new_lr,\n        'epoch_time_seconds': epoch_time,\n    })\n    \n    # Checkpointing\n    improved = val_loss < best_val_loss - CONFIG['min_delta']\n    \n    if improved:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'val_loss': val_loss,\n            'config': CONFIG,\n            'pretrain_enabled': ENABLE_PRETRAINING,\n        }, f'{CHECKPOINT_DIR}/best_model.pt')\n        marker = '* Best'\n        wandb.run.summary['best_val_loss'] = val_loss\n        wandb.run.summary['best_epoch'] = epoch + 1\n    else:\n        patience_counter += 1\n        marker = f'({patience_counter}/{PATIENCE})'\n    \n    print(f'Epoch {epoch+1:3d}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | LR: {new_lr:.2e} | {marker}')\n    \n    if patience_counter >= PATIENCE:\n        print(f'\\nEarly stopping at epoch {epoch+1}')\n        break\n    \n    if (epoch + 1) % 5 == 0:\n        torch.cuda.empty_cache()\n        gc.collect()\n\nelapsed = time.time() - start_time\nprint(f'\\nTraining complete in {elapsed/60:.1f} minutes')\nprint(f'Best validation loss: {best_val_loss:.6f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Evaluate and Scientific Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score, confusion_matrix\nimport numpy as np\n\n# Outbreak threshold (Norwegian regulatory threshold)\nOUTBREAK_THRESHOLD = 0.5\nCONFORMAL_COVERAGE = 0.90\n\n# Load best model\ncheckpoint = torch.load(f'{CHECKPOINT_DIR}/best_model.pt', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(f'Loaded best model from epoch {checkpoint[\"epoch\"]+1}')\n\n# =============================================================================\n# STEP 1: Wrap model with Conformal Prediction\n# =============================================================================\nprint('\\n' + '='*60)\nprint('CONFORMAL PREDICTION SETUP')\nprint('='*60)\n\nconformal_model = ConformalSeaLicePredictor(\n    base_model=model,\n    coverage=CONFORMAL_COVERAGE,\n    calibration_window=100,\n    use_adaptive=True,\n)\n\nprint(f'Target coverage: {CONFORMAL_COVERAGE*100:.0f}%')\n\n# =============================================================================\n# STEP 2: Calibrate on validation set\n# =============================================================================\nprint('\\nCalibrating conformal predictor on validation set...')\n\ncalib_preds = []\ncalib_targets = []\ncalib_masks = []\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader, desc='Calibrating'):\n        batch = {\n            'x': batch['x'].to(device),\n            'y': batch['y'].to(device),\n            'mask': batch['mask'].to(device),\n            'edge_index': edge_index_gpu,\n            'feature_indices': batch['feature_indices'],\n        }\n        output = model(batch)\n        calib_preds.append(output['predictions'].cpu())\n        calib_targets.append(batch['y'].cpu())\n        calib_masks.append(batch['mask'].cpu())\n\ncalib_preds = torch.cat(calib_preds, dim=0)\ncalib_targets = torch.cat(calib_targets, dim=0)\ncalib_masks = torch.cat(calib_masks, dim=0)\n\n# Flatten batch dimension for calibration\nB, T, N, C = calib_preds.shape\ncalib_preds = calib_preds.view(B*T, N, C)\ncalib_targets = calib_targets.view(B*T, N, C)\ncalib_masks = calib_masks.view(B*T, N)\n\nconformal_model.conformal.calibrate(calib_preds, calib_targets, calib_masks)\ncalib_diagnostics = conformal_model.conformal.get_diagnostics()\nprint(f'Calibration complete: {calib_diagnostics.get(\"n_residuals\", \"N/A\")} residuals collected')\n\n# =============================================================================\n# STEP 3: Evaluate on test set with uncertainty\n# =============================================================================\nprint('\\n' + '='*60)\nprint('TEST SET EVALUATION')\nprint('='*60)\n\nall_point_preds = []\nall_lower_bounds = []\nall_upper_bounds = []\nall_targets = []\nall_masks = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc='Testing'):\n        batch_x = batch['x'].to(device)\n        batch_y = batch['y']\n        batch_mask = batch['mask']\n        \n        # Process each sequence in the batch\n        for i in range(batch_x.shape[0]):\n            x_seq = batch_x[i]  # (T, N, F)\n            \n            interval = conformal_model.predict_with_uncertainty(\n                x_seq, edge_index_gpu, feature_indices=batch['feature_indices']\n            )\n            \n            all_point_preds.append(interval.point_prediction.cpu())\n            all_lower_bounds.append(interval.lower_bound.cpu())\n            all_upper_bounds.append(interval.upper_bound.cpu())\n            all_targets.append(batch_y[i])\n            all_masks.append(batch_mask[i])\n\n# Concatenate all\npoint_preds = torch.cat(all_point_preds, dim=0)\nlower_bounds = torch.cat(all_lower_bounds, dim=0)\nupper_bounds = torch.cat(all_upper_bounds, dim=0)\ntargets = torch.cat(all_targets, dim=0)\nmasks = torch.cat(all_masks, dim=0)\n\n# Extract adult female lice (index 0)\npred_af = point_preds[:, :, 0].numpy()\nlower_af = lower_bounds[:, :, 0].numpy()\nupper_af = upper_bounds[:, :, 0].numpy()\ntarget_af = targets[:, :, 0].numpy()\nmask_np = masks.numpy()\n\n# Flatten and filter by mask\npred_flat = pred_af[mask_np]\nlower_flat = lower_af[mask_np]\nupper_flat = upper_af[mask_np]\ntarget_flat = target_af[mask_np]\n\nprint(f'Valid observations: {len(pred_flat)}')\n\n# =============================================================================\n# STEP 4: Conformal Coverage Analysis\n# =============================================================================\nprint('\\n' + '='*60)\nprint('CONFORMAL PREDICTION COVERAGE')\nprint('='*60)\n\nin_interval = (target_flat >= lower_flat) & (target_flat <= upper_flat)\nempirical_coverage = in_interval.mean()\n\nprint(f'Target coverage: {CONFORMAL_COVERAGE*100:.0f}%')\nprint(f'Empirical coverage: {empirical_coverage*100:.1f}%')\nprint(f'Mean interval width: {(upper_flat - lower_flat).mean():.4f}')\n\n# =============================================================================\n# STEP 5: Regression Metrics\n# =============================================================================\nprint('\\n' + '='*60)\nprint('REGRESSION METRICS')\nprint('='*60)\n\nrmse = np.sqrt(np.mean((pred_flat - target_flat) ** 2))\nmae = np.mean(np.abs(pred_flat - target_flat))\n\nprint(f'RMSE: {rmse:.4f}')\nprint(f'MAE:  {mae:.4f}')\n\n# =============================================================================\n# STEP 6: Outbreak Detection - Point Predictions\n# =============================================================================\nprint('\\n' + '='*60)\nprint(f'OUTBREAK DETECTION - POINT PREDICTIONS (threshold={OUTBREAK_THRESHOLD})')\nprint('='*60)\n\ntarget_binary = (target_flat > OUTBREAK_THRESHOLD).astype(int)\nn_outbreaks = target_binary.sum()\nn_normal = len(target_binary) - n_outbreaks\n\nprint(f'Actual outbreaks: {n_outbreaks} ({100*n_outbreaks/len(target_binary):.1f}%)')\n\nif n_outbreaks > 0:\n    pred_binary_point = (pred_flat > OUTBREAK_THRESHOLD).astype(int)\n    precision_point = precision_score(target_binary, pred_binary_point, zero_division=0)\n    recall_point = recall_score(target_binary, pred_binary_point, zero_division=0)\n    f1_point = f1_score(target_binary, pred_binary_point, zero_division=0)\n    \n    print(f'Precision: {precision_point:.2%}')\n    print(f'Recall:    {recall_point:.2%}')\n    print(f'F1 Score:  {f1_point:.4f}')\nelse:\n    precision_point, recall_point, f1_point = 0, 0, 0\n    print('No outbreaks in test set!')\n\n# =============================================================================\n# STEP 7: Outbreak Detection - Risk-Aware (Upper Bounds)\n# =============================================================================\nprint('\\n' + '='*60)\nprint('OUTBREAK DETECTION - RISK-AWARE (Upper Bounds)')\nprint('='*60)\n\nif n_outbreaks > 0:\n    pred_binary_risk = (upper_flat > OUTBREAK_THRESHOLD).astype(int)\n    precision_risk = precision_score(target_binary, pred_binary_risk, zero_division=0)\n    recall_risk = recall_score(target_binary, pred_binary_risk, zero_division=0)\n    f1_risk = f1_score(target_binary, pred_binary_risk, zero_division=0)\n    \n    print(f'Precision: {precision_risk:.2%}')\n    print(f'Recall:    {recall_risk:.2%}')\n    print(f'F1 Score:  {f1_risk:.4f}')\n    \n    cm = confusion_matrix(target_binary, pred_binary_risk)\n    print(f'\\nConfusion Matrix:')\n    print(f'              Predicted')\n    print(f'            Normal  At-Risk')\n    print(f'  Normal     {cm[0,0]:5d}    {cm[0,1]:5d}')\n    print(f'  Outbreak   {cm[1,0]:5d}    {cm[1,1]:5d}')\nelse:\n    precision_risk, recall_risk, f1_risk = 0, 0, 0\n\n# =============================================================================\n# STEP 8: Scientific Audit\n# =============================================================================\nprint('\\n' + '='*60)\nprint('SCIENTIFIC AUDIT')\nprint('='*60)\n\nsample_batch = next(iter(val_loader))\nsample_batch = {\n    'x': sample_batch['x'].to(device),\n    'y': sample_batch['y'].to(device),\n    'mask': sample_batch['mask'].to(device),\n    'edge_index': edge_index_gpu,\n    'feature_indices': sample_batch['feature_indices'],\n}\n\n# Test 1: Counterfactual\nwith torch.no_grad():\n    output_orig = model(sample_batch)\n    growth_orig = (output_orig['predictions'][:, 1:] - output_orig['predictions'][:, :-1]).mean().item()\n    \n    x_hot = sample_batch['x'].clone()\n    x_hot[..., 0] += 5.0\n    batch_hot = {**sample_batch, 'x': x_hot}\n    output_hot = model(batch_hot)\n    growth_hot = (output_hot['predictions'][:, 1:] - output_hot['predictions'][:, :-1]).mean().item()\n\ntest1_pass = growth_hot > growth_orig\nprint(f'[TEST 1] Counterfactual: {\"PASS\" if test1_pass else \"FAIL\"}')\nprint(f'         Growth normal: {growth_orig:.6f}, Growth +5C: {growth_hot:.6f}')\n\n# Test 2: Long-horizon stability\nwith torch.no_grad():\n    x = sample_batch['x']\n    x_ext = x.repeat(1, 3, 1, 1)[:, :90]\n    model.network.reset_cache()\n    pred_ext, _ = model.network(x_ext, edge_index_gpu)\n    has_nan = torch.isnan(pred_ext).any().item()\n    has_inf = torch.isinf(pred_ext).any().item()\n\ntest2_pass = not (has_nan or has_inf)\nprint(f'[TEST 2] Long-horizon: {\"PASS\" if test2_pass else \"FAIL\"}')\n\n# Test 3: Graphon\nwith torch.no_grad():\n    model.network.reset_cache()\n    pred_n, _ = model.network(x, edge_index_gpu)\n    mean_n = pred_n.abs().mean().item()\n    \n    N = x.shape[2]\n    x_2n = x.repeat(1, 1, 2, 1)\n    edge_2n = torch.cat([edge_index_gpu, edge_index_gpu + N], dim=1)\n    model.network.reset_cache()\n    pred_2n, _ = model.network(x_2n, edge_2n)\n    mean_2n = pred_2n.abs().mean().item()\n    deviation = abs(mean_2n - mean_n) / (mean_n + 1e-8)\n\ntest3_pass = deviation < 0.10\nprint(f'[TEST 3] Graphon: {\"PASS\" if test3_pass else \"FAIL\"} ({100*deviation:.1f}% deviation)')\n\nprint(f'\\nAll tests: {\"PASSED\" if all([test1_pass, test2_pass, test3_pass]) else \"SOME FAILED\"}')\n\n# =============================================================================\n# STEP 9: Final Summary\n# =============================================================================\nprint('\\n' + '='*60)\nprint('FINAL SUMMARY')\nprint('='*60)\n\nprint(f'\\n--- Point Prediction Performance ---')\nprint(f'  Recall:    {recall_point:.1%} (target: 90%)')\nprint(f'  Precision: {precision_point:.1%} (target: 80%)')\nprint(f'  F1:        {f1_point:.4f} (target: 0.85)')\n\nprint(f'\\n--- Risk-Aware Performance (Upper Bound) ---')\nprint(f'  Recall:    {recall_risk:.1%} (target: 90%)')\nprint(f'  Precision: {precision_risk:.1%} (target: 80%)')\nprint(f'  F1:        {f1_risk:.4f} (target: 0.85)')\n\nprint(f'\\n--- Conformal Coverage ---')\nprint(f'  Target: {CONFORMAL_COVERAGE*100:.0f}%, Achieved: {empirical_coverage*100:.1f}%')\n\n# Log to wandb\nwandb.run.summary['outbreak_threshold'] = OUTBREAK_THRESHOLD\nwandb.run.summary['conformal_coverage_target'] = CONFORMAL_COVERAGE\nwandb.run.summary['conformal_coverage_achieved'] = float(empirical_coverage)\nwandb.run.summary['audit/counterfactual'] = test1_pass\nwandb.run.summary['audit/long_horizon'] = test2_pass\nwandb.run.summary['audit/graphon'] = test3_pass\nwandb.run.summary['test/rmse'] = float(rmse)\nwandb.run.summary['test/mae'] = float(mae)\nwandb.run.summary['test/precision_point'] = float(precision_point)\nwandb.run.summary['test/recall_point'] = float(recall_point)\nwandb.run.summary['test/f1_point'] = float(f1_point)\nwandb.run.summary['test/precision_risk'] = float(precision_risk)\nwandb.run.summary['test/recall_risk'] = float(recall_risk)\nwandb.run.summary['test/f1_risk'] = float(f1_risk)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Save Results and Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nOUTPUT_DIR = '/content/drive/MyDrive/GLKAN_Project/outputs'\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n\n# Save results\nresults = {\n    'timestamp': timestamp,\n    'config': CONFIG,\n    'outbreak_threshold': OUTBREAK_THRESHOLD,\n    'conformal_coverage': CONFORMAL_COVERAGE,\n    'training': {\n        'epochs': len(history['train_loss']),\n        'best_val_loss': best_val_loss,\n        'lr_schedule': history['lr'],\n    },\n    'regression': {'rmse': float(rmse), 'mae': float(mae)},\n    'point_prediction': {\n        'precision': float(precision_point),\n        'recall': float(recall_point),\n        'f1': float(f1_point),\n    },\n    'risk_aware': {\n        'precision': float(precision_risk),\n        'recall': float(recall_risk),\n        'f1': float(f1_risk),\n    },\n    'conformal': {\n        'target_coverage': CONFORMAL_COVERAGE,\n        'empirical_coverage': float(empirical_coverage),\n    },\n    'scientific_audit': {\n        'counterfactual': test1_pass,\n        'long_horizon': test2_pass,\n        'graphon': test3_pass,\n    },\n    'history': history,\n}\n\nresults_path = f'{OUTPUT_DIR}/results_{timestamp}.json'\nwith open(results_path, 'w') as f:\n    json.dump(results, f, indent=2, default=str)\n\n# Plot training curves\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].plot(history['train_loss'], label='Train')\naxes[0].plot(history['val_loss'], label='Val')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training Loss')\naxes[0].legend()\naxes[0].set_yscale('log')\n\naxes[1].plot(history['train_rmse'], label='Train')\naxes[1].plot(history['val_rmse'], label='Val')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('RMSE')\naxes[1].set_title('Training RMSE')\naxes[1].legend()\n\naxes[2].plot(history['lr'], label='Learning Rate', color='green')\naxes[2].set_xlabel('Epoch')\naxes[2].set_ylabel('Learning Rate')\naxes[2].set_title('Learning Rate Schedule')\naxes[2].set_yscale('log')\naxes[2].legend()\n\nplt.tight_layout()\nplot_path = f'{OUTPUT_DIR}/training_curves_{timestamp}.png'\nplt.savefig(plot_path, dpi=150)\nwandb.log({'training_curves': wandb.Image(fig)})\nplt.show()\n\n# Save model artifact\nartifact = wandb.Artifact('sealice-predictor-model', type='model')\nartifact.add_file(f'{CHECKPOINT_DIR}/best_model.pt')\nwandb.log_artifact(artifact)\n\nprint(f'Results saved to: {results_path}')\nprint(f'Training curves: {plot_path}')\n\n# Print summary\nprint('\\n' + '='*60)\nprint('FINAL SUMMARY')\nprint('='*60)\nprint(f'Model: SeaLicePredictor (with biological modules)')\nprint(f'Outbreak threshold: {OUTBREAK_THRESHOLD}')\nprint(f'Conformal coverage: {CONFORMAL_COVERAGE*100:.0f}% target, {empirical_coverage*100:.1f}% achieved')\nprint(f'\\nRegression:')\nprint(f'  RMSE: {rmse:.4f}')\nprint(f'  MAE:  {mae:.4f}')\nprint(f'\\nPoint Prediction:')\nprint(f'  Precision: {precision_point:.3f}')\nprint(f'  Recall:    {recall_point:.3f}')\nprint(f'  F1 Score:  {f1_point:.3f}')\nprint(f'\\nRisk-Aware (Upper Bound):')\nprint(f'  Precision: {precision_risk:.3f}')\nprint(f'  Recall:    {recall_risk:.3f}')\nprint(f'  F1 Score:  {f1_risk:.3f}')\nprint('='*60)\n\n# Finish wandb\nwandb.finish()\nprint('\\nwandb run finished!')"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}