{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-Liquid-KAN Sea Lice Prediction - A100 GPU Training\n",
    "## Phase 4: Production Training on Google Colab\n",
    "\n",
    "This notebook runs **A100-optimized GPU training** of the Graph-Liquid-KAN architecture with **Weights & Biases** integration for experiment tracking.\n",
    "\n",
    "**Architecture:**\n",
    "- **FastKAN Layers**: Gaussian RBF basis functions (learnable non-linearities)\n",
    "- **GraphonAggregator**: 1/N normalized message passing (scale invariant)\n",
    "- **LiquidKANCell**: Closed-form Continuous (CfC) dynamics with adaptive tau\n",
    "- **Physics-Informed Loss**: L_data + lambda_bio * L_bio\n",
    "\n",
    "**Target Metrics:**\n",
    "| Metric | Target | Description |\n",
    "|--------|--------|-------------|\n",
    "| Recall | >=90% | Catch 9/10 outbreaks |\n",
    "| Precision | >=80% | 8/10 predictions correct |\n",
    "| F1 Score | >=0.85 | Balance P/R |\n",
    "\n",
    "**Runtime Configuration:**\n",
    "- Runtime -> Change runtime type -> **A100 GPU**\n",
    "- Runtime -> Change runtime type -> **High RAM**\n",
    "\n",
    "**Setup:**\n",
    "1. Get wandb API key from https://wandb.ai/authorize\n",
    "2. Upload `glkan_data.zip` to Google Drive root\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/GLKAN_Project/checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/GLKAN_Project/outputs', exist_ok=True)\n",
    "print('Google Drive mounted and directories created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone the Graph-Liquid-KAN repository\n",
    "REPO_URL = 'https://github.com/themythicalyeti/graph-liquid-kan.git'\n",
    "REPO_DIR = '/content/graph-liquid-kan'\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f'Repository already exists at {REPO_DIR}')\n",
    "    %cd {REPO_DIR}\n",
    "    !git pull\n",
    "else:\n",
    "    !git clone {REPO_URL} {REPO_DIR}\n",
    "    %cd {REPO_DIR}\n",
    "\n",
    "print(f'\\nWorking directory: {os.getcwd()}')\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q torch torchvision --upgrade\n!pip install -q numpy pandas scipy scikit-learn\n!pip install -q loguru tqdm matplotlib\n!pip install -q torch-geometric\n!pip install -q wandb\n\nprint('\\nDependencies installed')\n\n# =============================================================================\n# WANDB AUTHENTICATION\n# =============================================================================\n# Option 1: Use Colab Secrets (recommended - no prompt each time)\n#   1. Click the key icon in left sidebar\n#   2. Add secret named \"WANDB_API_KEY\" with your key from https://wandb.ai/authorize\n#\n# Option 2: Manual login (will prompt for API key)\n#   Just run the cell - it will ask for your key\n\nimport wandb\nimport os\n\ntry:\n    from google.colab import userdata\n    WANDB_KEY = userdata.get('WANDB_API_KEY')\n    os.environ['WANDB_API_KEY'] = WANDB_KEY\n    wandb.login(key=WANDB_KEY)\n    print('Logged in to wandb using Colab Secrets')\nexcept:\n    print('Colab Secrets not configured - will prompt for API key')\n    print('Get your key at: https://wandb.ai/authorize')\n    wandb.login()\n    \nprint(f'wandb authenticated as: {wandb.api.viewer()[\"entity\"]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('GPU VERIFICATION')\n",
    "print('='*60)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # Enable TF32 for faster training on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    USE_AMP = True\n",
    "    print(f\"Mixed Precision (AMP): Enabled\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"[WARN] CUDA not available - using CPU (will be VERY slow)\")\n",
    "    print(\"[WARN] Please enable GPU: Runtime -> Change runtime type -> T4 GPU\")\n",
    "    device = torch.device('cpu')\n",
    "    USE_AMP = False\n",
    "\n",
    "print(f\"\\nDefault device: {device}\")\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Data from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Path to data on Drive\n",
    "DRIVE_DATA = '/content/drive/MyDrive/glkan_data.zip'\n",
    "LOCAL_DATA = '/content/data'\n",
    "\n",
    "if not os.path.exists(DRIVE_DATA):\n",
    "    print(f'ERROR: Data not found at {DRIVE_DATA}')\n",
    "    print('Please upload glkan_data.zip containing:')\n",
    "    print('  - tensors.npz (from Phase 2)')\n",
    "    print('  - spatial_graph.pt (from Phase 2)')\n",
    "else:\n",
    "    print('Extracting data...')\n",
    "    os.makedirs(LOCAL_DATA, exist_ok=True)\n",
    "    !unzip -q \"{DRIVE_DATA}\" -d {LOCAL_DATA}\n",
    "    !ls -la {LOCAL_DATA}\n",
    "    print('\\nData loaded')\n",
    "\n",
    "# Verify data\n",
    "TENSOR_PATH = f'{LOCAL_DATA}/tensors.npz'\n",
    "GRAPH_PATH = f'{LOCAL_DATA}/spatial_graph.pt'\n",
    "\n",
    "if os.path.exists(TENSOR_PATH) and os.path.exists(GRAPH_PATH):\n",
    "    data = np.load(TENSOR_PATH, allow_pickle=True)\n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"  X (features): {data['X'].shape}\")\n",
    "    print(f\"  Y (targets):  {data['Y'].shape}\")\n",
    "    print(f\"  mask:         {data['mask'].shape}\")\n",
    "    \n",
    "    graph = torch.load(GRAPH_PATH, weights_only=False)\n",
    "    print(f\"  edges:        {graph['edge_index'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Import GLKAN Architecture from Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '/content/graph-liquid-kan')\n\n# Import architecture from src/models\nfrom src.models import (\n    FastKAN,\n    GraphonAggregator,\n    LiquidKANCell,\n    GraphLiquidKANCell,\n    GLKANNetwork,\n    GLKANPredictor,\n)\n\n# Import training utilities from src/training\nfrom src.training import PhysicsInformedLoss, GLKANLoss\nfrom src.training.losses import LossConfig  # Import config class\n\n# Import dataset from src/data\nfrom src.data import SeaLiceGraphDataset\n\nprint('Imported from repository:')\nprint('  - FastKAN, GraphonAggregator, LiquidKANCell')\nprint('  - GraphLiquidKANCell, GLKANNetwork, GLKANPredictor')\nprint('  - PhysicsInformedLoss, GLKANLoss, LossConfig')\nprint('  - SeaLiceGraphDataset')\nprint('\\nGraph-Liquid-KAN architecture loaded from src/')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SeaLiceDataset(Dataset):\n",
    "    \"\"\"Dataset for GLKAN training.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, Y, mask, edge_index, window_size=30, stride=7, time_start=0, time_end=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.mask = mask\n",
    "        self.edge_index = edge_index\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        time_end = time_end or X.shape[0]\n",
    "        self.sequences = []\n",
    "        for t in range(time_start, time_end - window_size, stride):\n",
    "            self.sequences.append((t, t + window_size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t_start, t_end = self.sequences[idx]\n",
    "        return {\n",
    "            'x': self.X[t_start:t_end],\n",
    "            'y': self.Y[t_start:t_end],\n",
    "            'mask': self.mask[t_start:t_end],\n",
    "            'edge_index': self.edge_index,\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'x': torch.stack([b['x'] for b in batch]),\n",
    "        'y': torch.stack([b['y'] for b in batch]),\n",
    "        'mask': torch.stack([b['mask'] for b in batch]),\n",
    "        'edge_index': batch[0]['edge_index'],\n",
    "    }\n",
    "\n",
    "# Load data\n",
    "data = np.load(TENSOR_PATH, allow_pickle=True)\n",
    "graph = torch.load(GRAPH_PATH, weights_only=False)\n",
    "\n",
    "X = torch.from_numpy(data['X']).float()\n",
    "Y = torch.from_numpy(data['Y']).float()\n",
    "mask = torch.from_numpy(data['mask']).bool()\n",
    "edge_index = graph['edge_index']\n",
    "\n",
    "print(f'Data loaded:')\n",
    "print(f'  X: {X.shape}')\n",
    "print(f'  Y: {Y.shape}')\n",
    "print(f'  edge_index: {edge_index.shape}')\n",
    "\n",
    "# Train/Val/Test split (70/15/15)\n",
    "T_total = X.shape[0]\n",
    "T_train = int(T_total * 0.70)\n",
    "T_val = int(T_total * 0.85)\n",
    "\n",
    "# Configuration\n",
    "WINDOW_SIZE = 30\n",
    "STRIDE = 7\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_ds = SeaLiceDataset(X, Y, mask, edge_index, WINDOW_SIZE, STRIDE, 0, T_train)\n",
    "val_ds = SeaLiceDataset(X, Y, mask, edge_index, WINDOW_SIZE, STRIDE, T_train, T_val)\n",
    "test_ds = SeaLiceDataset(X, Y, mask, edge_index, WINDOW_SIZE, STRIDE, T_val, T_total)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                         collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'\\nDataLoaders created:')\n",
    "print(f'  Train sequences: {len(train_ds)}')\n",
    "print(f'  Val sequences: {len(val_ds)}')\n",
    "print(f'  Test sequences: {len(test_ds)}')\n",
    "print(f'  Nodes: {X.shape[1]}')\n",
    "print(f'  Edges: {edge_index.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Create Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# A100 Optimized Configuration\nCONFIG = {\n    'hidden_dim': 128,\n    'n_bases': 12,\n    'n_layers': 3,\n    'dropout': 0.15,\n    'lr': 1e-4,\n    'weight_decay': 1e-4,\n    'grad_clip': 1.0,\n    'epochs': 100,\n    'lambda_bio': 0.1,\n    'lambda_stability': 0.01,\n    'patience': 15,\n    'min_delta': 1e-6,\n    'n_nodes': X.shape[1],\n    'n_edges': edge_index.shape[1],\n    'n_features': X.shape[-1],\n    'window_size': WINDOW_SIZE,\n    'stride': STRIDE,\n    'batch_size': 16,\n}\n\nBATCH_SIZE = CONFIG['batch_size']\n\n# Initialize wandb run (already authenticated in Cell 2)\nprint('='*60)\nprint('INITIALIZING WANDB RUN')\nprint('='*60)\n\nrun = wandb.init(\n    project=\"graph-liquid-kan\",\n    name=f\"glkan-a100-{CONFIG['hidden_dim']}h-{CONFIG['n_layers']}L\",\n    config=CONFIG,\n    tags=[\"A100\", \"sea-lice\", \"graph-neural-network\", \"liquid-networks\"],\n)\n\nif torch.cuda.is_available():\n    wandb.config.update({\n        'gpu_name': torch.cuda.get_device_name(0),\n        'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9,\n        'mixed_precision': USE_AMP,\n    })\n\nprint(f'wandb run: {wandb.run.name}')\nprint(f'wandb URL: {wandb.run.get_url()}')\n\n# Move edge_index to GPU\nprint('\\n' + '='*60)\nprint('MOVING DATA TO GPU')\nprint('='*60)\n\nedge_index_gpu = edge_index.to(device)\nprint(f'  edge_index moved to: {edge_index_gpu.device}')\n\ntrain_ds.edge_index = edge_index_gpu\nval_ds.edge_index = edge_index_gpu\ntest_ds.edge_index = edge_index_gpu\n\n# Recreate dataloaders with num_workers=0\n# IMPORTANT: Cannot use num_workers>0 when edge_index is on GPU\n# because CUDA tensors cannot be pickled across worker processes\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n                          collate_fn=collate_fn, num_workers=0)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, \n                        collate_fn=collate_fn, num_workers=0)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, \n                         collate_fn=collate_fn, num_workers=0)\n\nprint(f'  DataLoaders recreated with num_workers=0 (required for GPU edge_index)')\n\n# Create model\ninput_dim = X.shape[-1]\noutput_dim = Y.shape[-1]\n\nmodel = GLKANPredictor(\n    input_dim=input_dim,\n    hidden_dim=CONFIG['hidden_dim'],\n    output_dim=output_dim,\n    n_bases=CONFIG['n_bases'],\n    n_layers=CONFIG['n_layers'],\n    dropout=CONFIG['dropout'],\n).to(device)\n\n# Optimizer and scheduler\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CONFIG['lr'],\n    weight_decay=CONFIG['weight_decay'],\n)\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=20, T_mult=2, eta_min=1e-7\n)\n\n# Create loss function with LossConfig\nloss_config = LossConfig(\n    lambda_bio=CONFIG['lambda_bio'],\n    lambda_stability=CONFIG['lambda_stability'],\n)\ncriterion = PhysicsInformedLoss(config=loss_config)\n\nscaler = torch.cuda.amp.GradScaler() if USE_AMP else None\n\nn_params = sum(p.numel() for p in model.parameters())\nwandb.config.update({'n_parameters': n_params})\nwandb.watch(model, log='all', log_freq=100)\n\nprint(f'\\nModel created:')\nprint(f'  Parameters: {n_params:,}')\nprint(f'  Device: {next(model.parameters()).device}')\nprint(f'  Hidden dim: {CONFIG[\"hidden_dim\"]}')\nprint(f'  Layers: {CONFIG[\"n_layers\"]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "EPOCHS = CONFIG['epochs']\n",
    "PATIENCE = CONFIG['patience']\n",
    "CHECKPOINT_DIR = '/content/drive/MyDrive/GLKAN_Project/checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_rmse': [], 'val_rmse': [], 'lr': []}\n",
    "\n",
    "print('='*60)\n",
    "print('GRAPH-LIQUID-KAN TRAINING')\n",
    "print('='*60)\n",
    "print(f'Device: {device}')\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'wandb: {wandb.run.get_url()}')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_rmse = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}', leave=False)\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        batch = {k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v \n",
    "                 for k, v in batch.items()}\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(batch)\n",
    "                loss, metrics = criterion(output['predictions'], batch['y'], batch['mask'])\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            output = model(batch)\n",
    "            loss, metrics = criterion(output['predictions'], batch['y'], batch['mask'])\n",
    "            \n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "            \n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mask_exp = batch['mask'].unsqueeze(-1).expand_as(output['predictions'])\n",
    "            sq_err = ((output['predictions'] - batch['y']) ** 2) * mask_exp.float()\n",
    "            rmse = torch.sqrt(sq_err.sum() / mask_exp.float().sum().clamp(min=1))\n",
    "            train_rmse += rmse.item()\n",
    "        \n",
    "        n_batches += 1\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            wandb.log({'batch/loss': loss.item(), 'batch/rmse': rmse.item()}, commit=False)\n",
    "    \n",
    "    train_loss /= max(n_batches, 1)\n",
    "    train_rmse /= max(n_batches, 1)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_rmse = 0\n",
    "    n_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = {k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v \n",
    "                     for k, v in batch.items()}\n",
    "            \n",
    "            if USE_AMP:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = model(batch)\n",
    "                    loss, _ = criterion(output['predictions'], batch['y'], batch['mask'])\n",
    "            else:\n",
    "                output = model(batch)\n",
    "                loss, _ = criterion(output['predictions'], batch['y'], batch['mask'])\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            mask_exp = batch['mask'].unsqueeze(-1).expand_as(output['predictions'])\n",
    "            sq_err = ((output['predictions'] - batch['y']) ** 2) * mask_exp.float()\n",
    "            rmse = torch.sqrt(sq_err.sum() / mask_exp.float().sum().clamp(min=1))\n",
    "            val_rmse += rmse.item()\n",
    "            n_val += 1\n",
    "    \n",
    "    val_loss /= max(n_val, 1)\n",
    "    val_rmse /= max(n_val, 1)\n",
    "    \n",
    "    scheduler.step()\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_rmse'].append(train_rmse)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    history['lr'].append(lr)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train/loss': train_loss,\n",
    "        'train/rmse': train_rmse,\n",
    "        'val/loss': val_loss,\n",
    "        'val/rmse': val_rmse,\n",
    "        'learning_rate': lr,\n",
    "        'epoch_time_seconds': epoch_time,\n",
    "    })\n",
    "    \n",
    "    # Checkpointing\n",
    "    improved = val_loss < best_val_loss - CONFIG['min_delta']\n",
    "    \n",
    "    if improved:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'config': CONFIG,\n",
    "        }, f'{CHECKPOINT_DIR}/best_model.pt')\n",
    "        marker = '* Best'\n",
    "        wandb.run.summary['best_val_loss'] = val_loss\n",
    "        wandb.run.summary['best_epoch'] = epoch + 1\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        marker = f'({patience_counter}/{PATIENCE})'\n",
    "    \n",
    "    print(f'Epoch {epoch+1:3d}/{EPOCHS} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | {marker}')\n",
    "    \n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f'\\nEarly stopping at epoch {epoch+1}')\n",
    "        break\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f'\\nTraining complete in {elapsed/60:.1f} minutes')\n",
    "print(f'Best validation loss: {best_val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Evaluate and Scientific Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(f'{CHECKPOINT_DIR}/best_model.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f'Loaded best model from epoch {checkpoint[\"epoch\"]+1}')\n",
    "\n",
    "# Evaluate on test set\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "all_masks = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(batch)\n",
    "        else:\n",
    "            output = model(batch)\n",
    "        \n",
    "        all_preds.append(output['predictions'].cpu())\n",
    "        all_targets.append(batch['y'].cpu())\n",
    "        all_masks.append(batch['mask'].cpu())\n",
    "\n",
    "preds = torch.cat(all_preds, dim=0)\n",
    "targets = torch.cat(all_targets, dim=0)\n",
    "masks = torch.cat(all_masks, dim=0)\n",
    "\n",
    "# Regression metrics\n",
    "pred_flat = preds[:, :, :, 0].numpy().flatten()\n",
    "target_flat = targets[:, :, :, 0].numpy().flatten()\n",
    "mask_flat = masks.numpy().flatten()\n",
    "\n",
    "pred_valid = pred_flat[mask_flat]\n",
    "target_valid = target_flat[mask_flat]\n",
    "\n",
    "rmse = np.sqrt(np.mean((pred_valid - target_valid) ** 2))\n",
    "mae = np.mean(np.abs(pred_valid - target_valid))\n",
    "\n",
    "print(f'\\nTest Set Results:')\n",
    "print(f'  RMSE: {rmse:.4f}')\n",
    "print(f'  MAE:  {mae:.4f}')\n",
    "\n",
    "# Scientific Audit\n",
    "print('\\n' + '='*60)\n",
    "print('SCIENTIFIC AUDIT')\n",
    "print('='*60)\n",
    "\n",
    "sample_batch = next(iter(val_loader))\n",
    "sample_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in sample_batch.items()}\n",
    "\n",
    "# Test 1: Counterfactual\n",
    "with torch.no_grad():\n",
    "    output_orig = model(sample_batch)\n",
    "    growth_orig = (output_orig['predictions'][:, 1:] - output_orig['predictions'][:, :-1]).mean().item()\n",
    "    \n",
    "    x_hot = sample_batch['x'].clone()\n",
    "    x_hot[..., 0] += 5.0\n",
    "    batch_hot = {**sample_batch, 'x': x_hot}\n",
    "    output_hot = model(batch_hot)\n",
    "    growth_hot = (output_hot['predictions'][:, 1:] - output_hot['predictions'][:, :-1]).mean().item()\n",
    "\n",
    "test1_pass = growth_hot > growth_orig\n",
    "print(f'[TEST 1] Counterfactual: {\"PASS\" if test1_pass else \"FAIL\"}')\n",
    "\n",
    "# Test 2: Long-horizon stability\n",
    "with torch.no_grad():\n",
    "    x = sample_batch['x']\n",
    "    x_ext = x.repeat(1, 3, 1, 1)[:, :90]\n",
    "    model.network.reset_cache()\n",
    "    pred_ext, _ = model.network(x_ext, sample_batch['edge_index'])\n",
    "    has_nan = torch.isnan(pred_ext).any().item()\n",
    "    has_inf = torch.isinf(pred_ext).any().item()\n",
    "\n",
    "test2_pass = not (has_nan or has_inf)\n",
    "print(f'[TEST 2] Long-horizon: {\"PASS\" if test2_pass else \"FAIL\"}')\n",
    "\n",
    "# Test 3: Graphon\n",
    "with torch.no_grad():\n",
    "    model.network.reset_cache()\n",
    "    pred_n, _ = model.network(x, sample_batch['edge_index'])\n",
    "    mean_n = pred_n.abs().mean().item()\n",
    "    \n",
    "    N = x.shape[2]\n",
    "    x_2n = x.repeat(1, 1, 2, 1)\n",
    "    edge_2n = torch.cat([sample_batch['edge_index'], sample_batch['edge_index'] + N], dim=1)\n",
    "    model.network.reset_cache()\n",
    "    pred_2n, _ = model.network(x_2n, edge_2n)\n",
    "    mean_2n = pred_2n.abs().mean().item()\n",
    "    deviation = abs(mean_2n - mean_n) / (mean_n + 1e-8)\n",
    "\n",
    "test3_pass = deviation < 0.10\n",
    "print(f'[TEST 3] Graphon: {\"PASS\" if test3_pass else \"FAIL\"} ({100*deviation:.1f}% deviation)')\n",
    "\n",
    "print(f'\\nAll tests: {\"PASSED\" if all([test1_pass, test2_pass, test3_pass]) else \"SOME FAILED\"}')\n",
    "\n",
    "# Log to wandb\n",
    "wandb.run.summary['audit/counterfactual'] = test1_pass\n",
    "wandb.run.summary['audit/long_horizon'] = test2_pass\n",
    "wandb.run.summary['audit/graphon'] = test3_pass\n",
    "wandb.run.summary['test/rmse'] = float(rmse)\n",
    "wandb.run.summary['test/mae'] = float(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Save Results and Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/GLKAN_Project/outputs'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'timestamp': timestamp,\n",
    "    'config': CONFIG,\n",
    "    'training': {\n",
    "        'epochs': len(history['train_loss']),\n",
    "        'best_val_loss': best_val_loss,\n",
    "    },\n",
    "    'regression': {'rmse': float(rmse), 'mae': float(mae)},\n",
    "    'scientific_audit': {\n",
    "        'counterfactual': test1_pass,\n",
    "        'long_horizon': test2_pass,\n",
    "        'graphon': test3_pass,\n",
    "    },\n",
    "    'history': history,\n",
    "}\n",
    "\n",
    "results_path = f'{OUTPUT_DIR}/results_{timestamp}.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "axes[1].plot(history['train_rmse'], label='Train')\n",
    "axes[1].plot(history['val_rmse'], label='Val')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].set_title('Training RMSE')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{OUTPUT_DIR}/training_curves_{timestamp}.png'\n",
    "plt.savefig(plot_path, dpi=150)\n",
    "wandb.log({'training_curves': wandb.Image(fig)})\n",
    "plt.show()\n",
    "\n",
    "# Save model artifact\n",
    "artifact = wandb.Artifact('glkan-model', type='model')\n",
    "artifact.add_file(f'{CHECKPOINT_DIR}/best_model.pt')\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "print(f'Results saved to: {results_path}')\n",
    "print(f'Training curves: {plot_path}')\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()\n",
    "print('\\nwandb run finished!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}