{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Graph-Liquid-KAN Sea Lice Prediction - Full GPU Training\n## Phase 4: Production Training on Google Colab\n\nThis notebook runs **full GPU training** of the Graph-Liquid-KAN architecture.\n\n**Training Configuration:**\n| Parameter | Value | Notes |\n|-----------|-------|-------|\n| Nodes | 1,777 | All farms |\n| Edges | 73,168 | Full connectivity |\n| Features | 8 | Environmental + treatment |\n| Epochs | 100 | With early stopping |\n| Batch Size | 8 | Optimized for GPU |\n| Mixed Precision | Yes | FP16 for speed |\n\n**Architecture:**\n- **FastKAN Layers**: Gaussian RBF basis functions (learnable non-linearities)\n- **GraphonAggregator**: 1/N normalized message passing (scale invariant)\n- **LiquidKANCell**: Closed-form Continuous (CfC) dynamics with adaptive tau\n- **Physics-Informed Loss**: L_data + Î»_bio * L_bio + Î»_stability * L_stability\n\n**Target Metrics:**\n| Metric | Target | Description |\n|--------|--------|-------------|\n| Recall | â‰¥90% | Catch 9/10 outbreaks |\n| Precision | â‰¥80% | 8/10 predictions correct |\n| F1 Score | â‰¥0.85 | Balance P/R |\n\n**Scientific Validation Tests:**\n1. **Counterfactual**: Temperature +5Â°C should increase lice growth\n2. **Long-Horizon**: 90-day rollout should remain stable\n3. **Graphon**: N vs 2N nodes should have <10% deviation\n\n**Prerequisites:**\n1. Google Colab (Pro recommended for longer training)\n2. Data uploaded to Google Drive as `glkan_data.zip`\n\n**Runtime Configuration:**\n- Runtime â†’ Change runtime type â†’ **T4 GPU** (or A100 for faster training)\n- Runtime â†’ Change runtime type â†’ **High RAM** (recommended)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/GLKAN_Project/checkpoints', exist_ok=True)\n",
    "os.makedirs('/content/drive/MyDrive/GLKAN_Project/outputs', exist_ok=True)\n",
    "print('âœ… Google Drive mounted and directories created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Verify GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!nvidia-smi\n\nimport torch\nimport gc\n\nprint('\\n' + '='*60)\nprint('GPU VERIFICATION')\nprint('='*60)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    print(f\"GPU: {gpu_name}\")\n    print(f\"VRAM: {gpu_memory:.1f} GB\")\n    device = torch.device('cuda')\n    \n    # Enable TF32 for faster training on Ampere GPUs\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    \n    # Check for mixed precision support\n    USE_AMP = True\n    print(f\"Mixed Precision (AMP): Enabled\")\n    \n    # Set memory-efficient options\n    torch.cuda.empty_cache()\n    gc.collect()\nelse:\n    print(\"[WARN] CUDA not available - using CPU (will be VERY slow)\")\n    print(\"[WARN] Please enable GPU: Runtime â†’ Change runtime type â†’ T4 GPU\")\n    device = torch.device('cpu')\n    USE_AMP = False\n\nprint(f\"\\nDefault device: {device}\")\nprint('='*60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision --upgrade\n",
    "!pip install -q numpy pandas scipy scikit-learn\n",
    "!pip install -q loguru tqdm matplotlib\n",
    "!pip install -q torch-geometric  # For graph operations\n",
    "\n",
    "print('\\nâœ… Dependencies installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Data from Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Path to data on Drive\n",
    "DRIVE_DATA = '/content/drive/MyDrive/glkan_data.zip'\n",
    "LOCAL_DATA = '/content/data'\n",
    "\n",
    "if not os.path.exists(DRIVE_DATA):\n",
    "    print(f'âŒ ERROR: Data not found at {DRIVE_DATA}')\n",
    "    print('Please upload glkan_data.zip containing:')\n",
    "    print('  - tensors.npz (from Phase 2)')\n",
    "    print('  - spatial_graph.pt (from Phase 2)')\n",
    "else:\n",
    "    print('Extracting data...')\n",
    "    os.makedirs(LOCAL_DATA, exist_ok=True)\n",
    "    !unzip -q \"{DRIVE_DATA}\" -d {LOCAL_DATA}\n",
    "    !ls -la {LOCAL_DATA}\n",
    "    print('\\nâœ… Data loaded')\n",
    "\n",
    "# Verify data\n",
    "TENSOR_PATH = f'{LOCAL_DATA}/tensors.npz'\n",
    "GRAPH_PATH = f'{LOCAL_DATA}/spatial_graph.pt'\n",
    "\n",
    "if os.path.exists(TENSOR_PATH) and os.path.exists(GRAPH_PATH):\n",
    "    data = np.load(TENSOR_PATH, allow_pickle=True)\n",
    "    print(f\"\\nData shapes:\")\n",
    "    print(f\"  X (features): {data['X'].shape}\")\n",
    "    print(f\"  Y (targets):  {data['Y'].shape}\")\n",
    "    print(f\"  mask:         {data['mask'].shape}\")\n",
    "    \n",
    "    graph = torch.load(GRAPH_PATH, weights_only=False)\n",
    "    print(f\"  edges:        {graph['edge_index'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Define Graph-Liquid-KAN Architecture\n",
    "\n",
    "Complete architecture from Phase 3 protocol:\n",
    "- FastKAN with RBF basis\n",
    "- Graphon-compliant aggregation\n",
    "- Liquid time-constant dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "# =============================================================================\n",
    "# FastKAN Layer - Gaussian RBF Basis Functions\n",
    "# =============================================================================\n",
    "class FastKAN(nn.Module):\n",
    "    \"\"\"KAN layer with Gaussian RBF basis functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, n_bases: int = 8):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.n_bases = n_bases\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(in_features)\n",
    "        \n",
    "        grid_centers = torch.linspace(-1.0, 1.0, n_bases)\n",
    "        self.register_buffer(\"grid_centers\", grid_centers)\n",
    "        \n",
    "        grid_spacing = 2.0 / (n_bases - 1)\n",
    "        sigma = grid_spacing / 2.0\n",
    "        self.register_buffer(\"gaussian_denom\", torch.tensor(2.0 * sigma * sigma))\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.empty(in_features, n_bases, out_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "        scale = 1.0 / math.sqrt(in_features * n_bases)\n",
    "        nn.init.uniform_(self.weights, -scale, scale)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.layer_norm(x)\n",
    "        x_expanded = x.unsqueeze(-1)\n",
    "        centers = self.grid_centers.view(1, 1, -1)\n",
    "        distances_sq = (x_expanded - centers) ** 2\n",
    "        basis = torch.exp(-distances_sq / self.gaussian_denom)\n",
    "        output = torch.einsum(\"...if,ifo->...o\", basis, self.weights)\n",
    "        return output + self.bias\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Graphon Aggregator - 1/N Normalized Message Passing\n",
    "# =============================================================================\n",
    "class GraphonAggregator(nn.Module):\n",
    "    \"\"\"Graph aggregator with 1/N normalization for scale invariance.\"\"\"\n",
    "    \n",
    "    def __init__(self, add_self_loops: bool = True):\n",
    "        super().__init__()\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self._cached_adj = None\n",
    "        self._cached_n = None\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 3:\n",
    "            return torch.stack([self._aggregate(x[b], edge_index) for b in range(x.shape[0])])\n",
    "        return self._aggregate(x, edge_index)\n",
    "    \n",
    "    def _aggregate(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        n_nodes = x.shape[0]\n",
    "        \n",
    "        if self._cached_adj is None or self._cached_n != n_nodes:\n",
    "            src, dst = edge_index[0], edge_index[1]\n",
    "            edge_weight = torch.ones(edge_index.shape[1], device=x.device)\n",
    "            \n",
    "            if self.add_self_loops:\n",
    "                loop_idx = torch.arange(n_nodes, device=x.device)\n",
    "                src = torch.cat([src, loop_idx])\n",
    "                dst = torch.cat([dst, loop_idx])\n",
    "                edge_weight = torch.cat([edge_weight, torch.ones(n_nodes, device=x.device)])\n",
    "            \n",
    "            degree = torch.zeros(n_nodes, device=x.device)\n",
    "            degree.scatter_add_(0, src, edge_weight)\n",
    "            degree = degree.clamp(min=1.0)\n",
    "            edge_weight = edge_weight / degree[src]\n",
    "            \n",
    "            indices = torch.stack([src, dst])\n",
    "            self._cached_adj = torch.sparse_coo_tensor(indices, edge_weight, (n_nodes, n_nodes)).coalesce()\n",
    "            self._cached_n = n_nodes\n",
    "        \n",
    "        return torch.sparse.mm(self._cached_adj, x)\n",
    "    \n",
    "    def reset_cache(self):\n",
    "        self._cached_adj = None\n",
    "        self._cached_n = None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Liquid-KAN Cell - CfC Dynamics\n",
    "# =============================================================================\n",
    "class LiquidKANCell(nn.Module):\n",
    "    \"\"\"Liquid time-constant cell with KAN-parameterized dynamics.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, n_bases: int = 8,\n",
    "                 tau_min: float = 0.01, tau_max: float = 10.0):\n",
    "        super().__init__()\n",
    "        self.tau_min = tau_min\n",
    "        self.tau_max = tau_max\n",
    "        \n",
    "        context_dim = input_dim + hidden_dim\n",
    "        self.kan_tau = FastKAN(context_dim, hidden_dim, n_bases)\n",
    "        self.kan_eq = FastKAN(context_dim, hidden_dim, n_bases)\n",
    "        self.kan_gate = FastKAN(context_dim, hidden_dim, n_bases)\n",
    "    \n",
    "    def forward(self, h: torch.Tensor, u: torch.Tensor, p: torch.Tensor, dt: torch.Tensor) -> torch.Tensor:\n",
    "        context = torch.cat([u, p], dim=-1)\n",
    "        \n",
    "        tau = F.softplus(self.kan_tau(context)) + self.tau_min\n",
    "        tau = torch.clamp(tau, self.tau_min, self.tau_max)\n",
    "        \n",
    "        x_eq = self.kan_eq(context)\n",
    "        gate = torch.sigmoid(self.kan_gate(context))\n",
    "        \n",
    "        if isinstance(dt, (int, float)):\n",
    "            dt = torch.tensor(dt, device=h.device, dtype=h.dtype)\n",
    "        while dt.dim() < tau.dim():\n",
    "            dt = dt.unsqueeze(-1)\n",
    "        \n",
    "        decay = torch.exp(-dt / tau)\n",
    "        return decay * h + (1 - decay) * x_eq * gate\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Graph-Liquid-KAN Cell\n",
    "# =============================================================================\n",
    "class GraphLiquidKANCell(nn.Module):\n",
    "    \"\"\"Combines graph aggregation with liquid dynamics.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, n_bases: int = 8):\n",
    "        super().__init__()\n",
    "        self.aggregator = GraphonAggregator()\n",
    "        self.pressure_proj = FastKAN(hidden_dim, hidden_dim, n_bases)\n",
    "        self.liquid_cell = LiquidKANCell(input_dim, hidden_dim, n_bases)\n",
    "    \n",
    "    def forward(self, h: torch.Tensor, u: torch.Tensor, edge_index: torch.Tensor, dt: torch.Tensor) -> torch.Tensor:\n",
    "        h_agg = self.aggregator(h, edge_index)\n",
    "        p = self.pressure_proj(h_agg)\n",
    "        return self.liquid_cell(h, u, p, dt)\n",
    "    \n",
    "    def reset_cache(self):\n",
    "        self.aggregator.reset_cache()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Complete GLKAN Network\n",
    "# =============================================================================\n",
    "class GLKANNetwork(nn.Module):\n",
    "    \"\"\"Full Graph-Liquid-KAN network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, output_dim: int = 3,\n",
    "                 n_bases: int = 8, n_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.input_encoder = FastKAN(input_dim, hidden_dim, n_bases)\n",
    "        self.cells = nn.ModuleList([GraphLiquidKANCell(hidden_dim, hidden_dim, n_bases) for _ in range(n_layers)])\n",
    "        self.layer_norms = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_decoder = FastKAN(hidden_dim, output_dim, n_bases)\n",
    "        self.h0 = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "            squeeze = True\n",
    "        else:\n",
    "            squeeze = False\n",
    "        \n",
    "        B, T, N, F = x.shape\n",
    "        dt = torch.ones(T, device=x.device) / T\n",
    "        \n",
    "        h_list = [self.h0.expand(B, N, -1).clone() for _ in range(self.n_layers)]\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            u_t = self.dropout(self.input_encoder(x[:, t]))\n",
    "            \n",
    "            for i in range(self.n_layers):\n",
    "                h_new = self.cells[i](h_list[i], u_t if i == 0 else h_list[i-1], edge_index, dt[t])\n",
    "                h_new = self.layer_norms[i](h_new)\n",
    "                if i > 0:\n",
    "                    h_new = h_new + h_list[i]\n",
    "                h_list[i] = self.dropout(h_new)\n",
    "            \n",
    "            outputs.append(self.output_decoder(h_list[-1]))\n",
    "        \n",
    "        pred = torch.stack(outputs, dim=1)\n",
    "        if squeeze:\n",
    "            pred = pred.squeeze(0)\n",
    "        return pred, h_list[-1]\n",
    "    \n",
    "    def reset_cache(self):\n",
    "        for cell in self.cells:\n",
    "            cell.reset_cache()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# GLKAN Predictor (Training Wrapper)\n",
    "# =============================================================================\n",
    "class GLKANPredictor(nn.Module):\n",
    "    \"\"\"Predictor wrapper for training.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, output_dim: int = 3,\n",
    "                 n_bases: int = 8, n_layers: int = 1, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.network = GLKANNetwork(input_dim, hidden_dim, output_dim, n_bases, n_layers, dropout)\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def forward(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        pred, _ = self.network(batch['x'], batch['edge_index'])\n",
    "        return {'predictions': pred}\n",
    "\n",
    "print('âœ… Graph-Liquid-KAN architecture defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Define Physics-Informed Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Physics-Informed Loss for GLKAN.\n",
    "    \n",
    "    L = L_data + Î»_bio * L_bio + Î»_stability * L_stability\n",
    "    \n",
    "    Components:\n",
    "    - L_data: Huber loss on masked observations\n",
    "    - L_bio: Non-negativity + growth rate bounds\n",
    "    - L_stability: Tau regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_bio: float = 0.1, lambda_stability: float = 0.01,\n",
    "                 max_daily_change: float = 0.2, huber_delta: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_bio = lambda_bio\n",
    "        self.lambda_stability = lambda_stability\n",
    "        self.max_daily_change = max_daily_change\n",
    "        self.huber = nn.SmoothL1Loss(reduction='none', beta=huber_delta)\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> Tuple[torch.Tensor, Dict]:\n",
    "        metrics = {}\n",
    "        \n",
    "        # L_data: Huber loss on masked observations\n",
    "        if mask.dim() == pred.dim() - 1:\n",
    "            mask = mask.unsqueeze(-1).expand_as(pred)\n",
    "        \n",
    "        huber = self.huber(pred, target)\n",
    "        masked_huber = huber * mask.float()\n",
    "        n_valid = mask.float().sum()\n",
    "        l_data = masked_huber.sum() / n_valid.clamp(min=1)\n",
    "        metrics['l_data'] = l_data.item()\n",
    "        \n",
    "        # L_bio: Non-negativity + growth rate\n",
    "        l_nonneg = F.relu(-pred).mean()\n",
    "        \n",
    "        if pred.shape[1] > 1:\n",
    "            delta = pred[:, 1:] - pred[:, :-1]\n",
    "            threshold = self.max_daily_change * (pred[:, :-1].abs() + 0.1)\n",
    "            l_growth = F.relu(delta.abs() - threshold).mean()\n",
    "        else:\n",
    "            l_growth = torch.tensor(0.0, device=pred.device)\n",
    "        \n",
    "        l_bio = l_nonneg + 0.1 * l_growth\n",
    "        metrics['l_bio'] = l_bio.item()\n",
    "        metrics['l_nonneg'] = l_nonneg.item()\n",
    "        metrics['l_growth'] = l_growth.item()\n",
    "        \n",
    "        # Total loss\n",
    "        total = l_data + self.lambda_bio * l_bio\n",
    "        metrics['total_loss'] = total.item()\n",
    "        \n",
    "        return total, metrics\n",
    "\n",
    "print('âœ… Physics-Informed Loss defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import Dataset, DataLoader\n\nclass SeaLiceDataset(Dataset):\n    \"\"\"Dataset for GLKAN training.\"\"\"\n    \n    def __init__(self, X, Y, mask, edge_index, window_size=30, stride=7, time_start=0, time_end=None):\n        self.X = X\n        self.Y = Y\n        self.mask = mask\n        self.edge_index = edge_index\n        self.window_size = window_size\n        \n        time_end = time_end or X.shape[0]\n        self.sequences = []\n        for t in range(time_start, time_end - window_size, stride):\n            self.sequences.append((t, t + window_size))\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        t_start, t_end = self.sequences[idx]\n        return {\n            'x': self.X[t_start:t_end],\n            'y': self.Y[t_start:t_end],\n            'mask': self.mask[t_start:t_end],\n            'edge_index': self.edge_index,\n        }\n\ndef collate_fn(batch):\n    return {\n        'x': torch.stack([b['x'] for b in batch]),\n        'y': torch.stack([b['y'] for b in batch]),\n        'mask': torch.stack([b['mask'] for b in batch]),\n        'edge_index': batch[0]['edge_index'],\n    }\n\n# Load data\ndata = np.load(TENSOR_PATH, allow_pickle=True)\ngraph = torch.load(GRAPH_PATH, weights_only=False)\n\nX = torch.from_numpy(data['X']).float()\nY = torch.from_numpy(data['Y']).float()\nmask = torch.from_numpy(data['mask']).bool()\nedge_index = graph['edge_index']\n\n# Train/Val/Test split (70/15/15)\nT_total = X.shape[0]\nT_train = int(T_total * 0.70)\nT_val = int(T_total * 0.85)\n\n# Configuration - OPTIMIZED FOR FULL GPU TRAINING\nWINDOW_SIZE = 30  # 30-day sequences\nSTRIDE = 7        # Weekly stride\nBATCH_SIZE = 8    # Larger batch for GPU (adjust if OOM)\n\ntrain_ds = SeaLiceDataset(X, Y, mask, edge_index, WINDOW_SIZE, STRIDE, 0, T_train)\nval_ds = SeaLiceDataset(X, Y, mask, edge_index, WINDOW_SIZE, STRIDE, T_train, T_val)\ntest_ds = SeaLiceDataset(X, Y, mask, edge_index, WINDOW_SIZE, STRIDE, T_val, T_total)\n\n# Use pin_memory for faster GPU transfer\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n                          collate_fn=collate_fn, num_workers=2, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, \n                        collate_fn=collate_fn, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, \n                         collate_fn=collate_fn, num_workers=2, pin_memory=True)\n\nprint(f'\\nâœ… DataLoaders created (FULL DATA):')\nprint(f'  Train sequences: {len(train_ds)}')\nprint(f'  Val sequences: {len(val_ds)}')\nprint(f'  Test sequences: {len(test_ds)}')\nprint(f'  Batch size: {BATCH_SIZE}')\nprint(f'  Window size: {WINDOW_SIZE}')\nprint(f'  Nodes: {X.shape[1]} (ALL FARMS)')\nprint(f'  Edges: {edge_index.shape[1]}')\nprint(f'  Features: {X.shape[2]}')\nprint(f'\\n  Estimated batches/epoch: {len(train_loader)}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Create Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# PRODUCTION TRAINING CONFIGURATION\n# =============================================================================\nCONFIG = {\n    # Model architecture\n    'hidden_dim': 64,\n    'n_bases': 8,\n    'n_layers': 2,       # 2 layers for better capacity\n    'dropout': 0.1,\n    \n    # Training hyperparameters\n    'lr': 1e-4,\n    'weight_decay': 1e-4,\n    'grad_clip': 1.0,\n    'epochs': 100,       # Full training\n    \n    # Loss weights\n    'lambda_bio': 0.1,\n    'lambda_stability': 0.01,\n    \n    # Early stopping\n    'patience': 15,      # Stop if no improvement for 15 epochs\n    'min_delta': 1e-6,   # Minimum improvement to count\n}\n\n# Create model\ninput_dim = X.shape[-1]\noutput_dim = Y.shape[-1]\n\nmodel = GLKANPredictor(\n    input_dim=input_dim,\n    hidden_dim=CONFIG['hidden_dim'],\n    output_dim=output_dim,\n    n_bases=CONFIG['n_bases'],\n    n_layers=CONFIG['n_layers'],\n    dropout=CONFIG['dropout'],\n).to(device)\n\n# Create optimizer with weight decay\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CONFIG['lr'],\n    weight_decay=CONFIG['weight_decay'],\n    betas=(0.9, 0.999),\n)\n\n# Cosine annealing with warm restarts for better convergence\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=20, T_mult=2, eta_min=1e-7\n)\n\n# Create loss function\ncriterion = PhysicsInformedLoss(\n    lambda_bio=CONFIG['lambda_bio'],\n    lambda_stability=CONFIG['lambda_stability'],\n)\n\n# Mixed precision scaler\nscaler = torch.cuda.amp.GradScaler() if USE_AMP else None\n\nn_params = sum(p.numel() for p in model.parameters())\nprint(f'\\nâœ… Model created for FULL GPU TRAINING:')\nprint(f'  Parameters: {n_params:,}')\nprint(f'  Device: {device}')\nprint(f'  Hidden dim: {CONFIG[\"hidden_dim\"]}')\nprint(f'  RBF bases: {CONFIG[\"n_bases\"]}')\nprint(f'  Layers: {CONFIG[\"n_layers\"]}')\nprint(f'  Mixed Precision: {USE_AMP}')\nprint(f'  Epochs: {CONFIG[\"epochs\"]}')\nprint(f'  Early Stopping Patience: {CONFIG[\"patience\"]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 9: Full GPU Training Loop\n\n**Features:**\n- Mixed precision (FP16) for faster training\n- Early stopping with patience\n- Gradient clipping for stability\n- Periodic checkpointing\n- Memory-efficient batch processing"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from tqdm.auto import tqdm\nimport time\n\n# Training settings\nEPOCHS = CONFIG['epochs']\nPATIENCE = CONFIG['patience']\nCHECKPOINT_DIR = '/content/drive/MyDrive/GLKAN_Project/checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\nbest_val_loss = float('inf')\npatience_counter = 0\nhistory = {'train_loss': [], 'val_loss': [], 'train_rmse': [], 'val_rmse': [], 'lr': []}\n\nprint('='*60)\nprint('GRAPH-LIQUID-KAN FULL GPU TRAINING')\nprint('='*60)\nprint(f'Device: {device}')\nprint(f'Mixed Precision: {USE_AMP}')\nprint(f'Epochs: {EPOCHS}')\nprint(f'Early Stopping Patience: {PATIENCE}')\nprint(f'Nodes: {X.shape[1]} | Edges: {edge_index.shape[1]}')\nprint(f'Checkpoints: {CHECKPOINT_DIR}')\nprint('='*60 + '\\n')\n\nstart_time = time.time()\n\nfor epoch in range(EPOCHS):\n    epoch_start = time.time()\n    \n    # =========================================================================\n    # Training\n    # =========================================================================\n    model.train()\n    train_loss = 0\n    train_rmse = 0\n    n_batches = 0\n    \n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Train]', leave=False)\n    for batch in pbar:\n        # Move to GPU\n        batch = {k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v \n                 for k, v in batch.items()}\n        \n        optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n        \n        # Mixed precision forward pass\n        if USE_AMP:\n            with torch.cuda.amp.autocast():\n                output = model(batch)\n                loss, metrics = criterion(output['predictions'], batch['y'], batch['mask'])\n            \n            if torch.isnan(loss):\n                continue\n            \n            # Scaled backward pass\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            output = model(batch)\n            loss, metrics = criterion(output['predictions'], batch['y'], batch['mask'])\n            \n            if torch.isnan(loss):\n                continue\n            \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['grad_clip'])\n            optimizer.step()\n        \n        train_loss += loss.item()\n        \n        # Compute RMSE\n        with torch.no_grad():\n            mask_exp = batch['mask'].unsqueeze(-1).expand_as(output['predictions'])\n            sq_err = ((output['predictions'] - batch['y']) ** 2) * mask_exp.float()\n            rmse = torch.sqrt(sq_err.sum() / mask_exp.float().sum().clamp(min=1))\n            train_rmse += rmse.item()\n        \n        n_batches += 1\n        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'rmse': f'{rmse.item():.4f}'})\n    \n    train_loss /= max(n_batches, 1)\n    train_rmse /= max(n_batches, 1)\n    \n    # =========================================================================\n    # Validation\n    # =========================================================================\n    model.eval()\n    val_loss = 0\n    val_rmse = 0\n    n_val = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1}/{EPOCHS} [Val]', leave=False):\n            batch = {k: v.to(device, non_blocking=True) if isinstance(v, torch.Tensor) else v \n                     for k, v in batch.items()}\n            \n            if USE_AMP:\n                with torch.cuda.amp.autocast():\n                    output = model(batch)\n                    loss, _ = criterion(output['predictions'], batch['y'], batch['mask'])\n            else:\n                output = model(batch)\n                loss, _ = criterion(output['predictions'], batch['y'], batch['mask'])\n            \n            val_loss += loss.item()\n            \n            mask_exp = batch['mask'].unsqueeze(-1).expand_as(output['predictions'])\n            sq_err = ((output['predictions'] - batch['y']) ** 2) * mask_exp.float()\n            rmse = torch.sqrt(sq_err.sum() / mask_exp.float().sum().clamp(min=1))\n            val_rmse += rmse.item()\n            n_val += 1\n    \n    val_loss /= max(n_val, 1)\n    val_rmse /= max(n_val, 1)\n    \n    # Update scheduler\n    scheduler.step()\n    \n    # Save history\n    lr = optimizer.param_groups[0]['lr']\n    history['train_loss'].append(train_loss)\n    history['val_loss'].append(val_loss)\n    history['train_rmse'].append(train_rmse)\n    history['val_rmse'].append(val_rmse)\n    history['lr'].append(lr)\n    \n    epoch_time = time.time() - epoch_start\n    \n    # =========================================================================\n    # Checkpointing & Early Stopping\n    # =========================================================================\n    improved = val_loss < best_val_loss - CONFIG['min_delta']\n    \n    if improved:\n        best_val_loss = val_loss\n        patience_counter = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n            'config': CONFIG,\n        }, f'{CHECKPOINT_DIR}/best_model.pt')\n        marker = 'âœ“ Best'\n    else:\n        patience_counter += 1\n        marker = f'({patience_counter}/{PATIENCE})'\n    \n    # Periodic checkpoint every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'history': history,\n        }, f'{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pt')\n    \n    # Print progress\n    print(f'Epoch {epoch+1:3d}/{EPOCHS} | '\n          f'Train: {train_loss:.4f} | Val: {val_loss:.4f} | '\n          f'RMSE: {val_rmse:.4f} | LR: {lr:.2e} | '\n          f'Time: {epoch_time:.1f}s | {marker}')\n    \n    # Early stopping check\n    if patience_counter >= PATIENCE:\n        print(f'\\nâš ï¸ Early stopping triggered after {epoch+1} epochs')\n        print(f'   No improvement for {PATIENCE} consecutive epochs')\n        break\n    \n    # Clear GPU cache periodically\n    if (epoch + 1) % 5 == 0:\n        torch.cuda.empty_cache()\n        gc.collect()\n\nelapsed = time.time() - start_time\nprint(f'\\n{\"=\"*60}')\nprint(f'âœ… TRAINING COMPLETE')\nprint(f'{\"=\"*60}')\nprint(f'Total time: {elapsed/60:.1f} minutes ({elapsed/3600:.2f} hours)')\nprint(f'Epochs completed: {epoch+1}')\nprint(f'Best validation loss: {best_val_loss:.6f}')\nprint(f'Best model saved to: {CHECKPOINT_DIR}/best_model.pt')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Evaluate Outbreak Detection\n",
    "\n",
    "**Target: 90% Recall, 80% Precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import precision_recall_curve, f1_score, precision_score, recall_score, confusion_matrix\n\nOUTBREAK_THRESHOLD = 0.5  # Norwegian regulatory limit\n\n# Load best model\ncheckpoint = torch.load(f'{CHECKPOINT_DIR}/best_model.pt', map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\nprint(f'Loaded best model from epoch {checkpoint[\"epoch\"]+1}')\nprint(f'Validation loss: {checkpoint[\"val_loss\"]:.6f}')\n\n# Collect predictions on TEST set (held out data)\nall_preds = []\nall_targets = []\nall_masks = []\n\nprint(f'\\nEvaluating on TEST set ({len(test_loader)} batches)...')\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc='Testing'):\n        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        \n        if USE_AMP:\n            with torch.cuda.amp.autocast():\n                output = model(batch)\n        else:\n            output = model(batch)\n        \n        all_preds.append(output['predictions'].cpu())\n        all_targets.append(batch['y'].cpu())\n        all_masks.append(batch['mask'].cpu())\n\npreds = torch.cat(all_preds, dim=0)\ntargets = torch.cat(all_targets, dim=0)\nmasks = torch.cat(all_masks, dim=0)\n\n# Extract adult female lice (index 0) with mask\npred_af = preds[:, :, :, 0].numpy().flatten()\ntarget_af = targets[:, :, :, 0].numpy().flatten()\nmask_flat = masks.numpy().flatten()\n\npred_valid = pred_af[mask_flat]\ntarget_valid = target_af[mask_flat]\n\nprint(f'\\n{\"=\"*60}')\nprint('OUTBREAK DETECTION EVALUATION (TEST SET)')\nprint(f'{\"=\"*60}')\nprint(f'Valid observations: {len(pred_valid)}')\nprint(f'Outbreak threshold: {OUTBREAK_THRESHOLD}')\n\n# Regression metrics\nrmse = np.sqrt(np.mean((pred_valid - target_valid) ** 2))\nmae = np.mean(np.abs(pred_valid - target_valid))\ncorr = np.corrcoef(pred_valid, target_valid)[0, 1] if len(pred_valid) > 1 else 0\n\nprint(f'\\nRegression Metrics:')\nprint(f'  RMSE:        {rmse:.4f}')\nprint(f'  MAE:         {mae:.4f}')\nprint(f'  Correlation: {corr:.4f}')\nprint(f'  Pred range:   [{pred_valid.min():.3f}, {pred_valid.max():.3f}]')\nprint(f'  Target range: [{target_valid.min():.3f}, {target_valid.max():.3f}]')\nprint(f'  Pred std:     {pred_valid.std():.4f}')\nprint(f'  Target std:   {target_valid.std():.4f}')\n\n# Classification metrics\ntarget_binary = (target_valid > OUTBREAK_THRESHOLD).astype(int)\nn_outbreaks = target_binary.sum()\nprint(f'\\nOutbreak Distribution:')\nprint(f'  Outbreaks: {n_outbreaks} ({100*n_outbreaks/len(target_binary):.1f}%)')\nprint(f'  Normal:    {len(target_binary)-n_outbreaks} ({100*(1-n_outbreaks/len(target_binary)):.1f}%)')\n\nif n_outbreaks > 0:\n    # Find optimal threshold for 90% recall\n    precisions, recalls, thresholds = precision_recall_curve(target_binary, pred_valid)\n    \n    # Find best threshold achieving target recall\n    best_thresh = None\n    best_f1 = 0\n    target_recall = 0.90\n    \n    for p, r, t in zip(precisions[:-1], recalls[:-1], thresholds):\n        if r >= target_recall:\n            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0\n            if f1 > best_f1:\n                best_f1 = f1\n                best_thresh = t\n                best_p = p\n                best_r = r\n    \n    if best_thresh:\n        pred_binary = (pred_valid > best_thresh).astype(int)\n        precision = precision_score(target_binary, pred_binary, zero_division=0)\n        recall = recall_score(target_binary, pred_binary, zero_division=0)\n        f1 = f1_score(target_binary, pred_binary, zero_division=0)\n        cm = confusion_matrix(target_binary, pred_binary)\n        \n        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (cm[0,0], 0, 0, 0)\n        \n        print(f'\\n{\"=\"*60}')\n        print(f'OUTBREAK DETECTION @ threshold={best_thresh:.4f}')\n        print(f'{\"=\"*60}')\n        print(f'  Precision: {precision:.2%} (target: â‰¥80%)')\n        print(f'  Recall:    {recall:.2%} (target: â‰¥90%)')\n        print(f'  F1 Score:  {f1:.4f} (target: â‰¥0.85)')\n        \n        print(f'\\nConfusion Matrix:')\n        print(f'                    Predicted')\n        print(f'                 Normal  Outbreak')\n        print(f'  Actual Normal   {tn:5d}    {fp:5d}')\n        print(f'  Actual Outbreak {fn:5d}    {tp:5d}')\n        \n        # Check targets\n        print(f'\\n{\"=\"*60}')\n        print('TARGET ASSESSMENT')\n        print(f'{\"=\"*60}')\n        recall_pass = recall >= 0.90\n        precision_pass = precision >= 0.80\n        f1_pass = f1 >= 0.85\n        \n        print(f'  Recall â‰¥ 90%:    {\"âœ… PASS\" if recall_pass else \"âŒ FAIL\"} ({recall:.1%})')\n        print(f'  Precision â‰¥ 80%: {\"âœ… PASS\" if precision_pass else \"âŒ FAIL\"} ({precision:.1%})')\n        print(f'  F1 â‰¥ 0.85:       {\"âœ… PASS\" if f1_pass else \"âŒ FAIL\"} ({f1:.4f})')\n        \n        if recall_pass and precision_pass:\n            print('\\nðŸŽ‰ MODEL MEETS OUTBREAK DETECTION TARGETS!')\n        else:\n            print('\\nâš ï¸ Model needs more training or tuning')\n    else:\n        print(f'\\n[WARN] Could not achieve {target_recall:.0%} recall with any threshold')\nelse:\n    print('\\n[WARN] No outbreaks in test set - cannot compute detection metrics')\n    print('       This may indicate data imbalance issues')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Scientific Validation Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('SCIENTIFIC VALIDATION AUDIT')\n",
    "print('='*60)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get a sample batch\n",
    "sample_batch = next(iter(val_loader))\n",
    "sample_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in sample_batch.items()}\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 1: Counterfactual (Temperature Effect)\n",
    "# =============================================================================\n",
    "print('\\n[TEST 1] Counterfactual: Temperature Effect')\n",
    "print('-' * 40)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Original prediction\n",
    "    output_orig = model(sample_batch)\n",
    "    pred_orig = output_orig['predictions']\n",
    "    \n",
    "    if pred_orig.shape[1] > 1:\n",
    "        growth_orig = (pred_orig[:, 1:] - pred_orig[:, :-1]).mean().item()\n",
    "    else:\n",
    "        growth_orig = 0\n",
    "    \n",
    "    # +5Â°C temperature\n",
    "    x_hot = sample_batch['x'].clone()\n",
    "    x_hot[..., 0] += 5.0  # Temperature is feature 0\n",
    "    batch_hot = {**sample_batch, 'x': x_hot}\n",
    "    \n",
    "    output_hot = model(batch_hot)\n",
    "    pred_hot = output_hot['predictions']\n",
    "    \n",
    "    if pred_hot.shape[1] > 1:\n",
    "        growth_hot = (pred_hot[:, 1:] - pred_hot[:, :-1]).mean().item()\n",
    "    else:\n",
    "        growth_hot = 0\n",
    "\n",
    "print(f'  Original growth rate: {growth_orig:.6f}')\n",
    "print(f'  +5Â°C growth rate:     {growth_hot:.6f}')\n",
    "print(f'  Difference:           {growth_hot - growth_orig:.6f}')\n",
    "\n",
    "test1_pass = growth_hot > growth_orig\n",
    "print(f'  Result: {\"[PASS]\" if test1_pass else \"[FAIL]\"} Temperature {\"increases\" if test1_pass else \"does not increase\"} growth')\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 2: Long-Horizon Stability\n",
    "# =============================================================================\n",
    "print('\\n[TEST 2] Long-Horizon Stability (90 days)')\n",
    "print('-' * 40)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = sample_batch['x']\n",
    "    B, T, N, F = x.shape\n",
    "    \n",
    "    # Extend to 90 days\n",
    "    n_repeats = (90 // T) + 2\n",
    "    x_extended = x.repeat(1, n_repeats, 1, 1)[:, :90]\n",
    "    \n",
    "    batch_ext = {'x': x_extended, 'edge_index': sample_batch['edge_index']}\n",
    "    pred_ext, _ = model.network(x_extended, sample_batch['edge_index'])\n",
    "    \n",
    "    pred_min = pred_ext.min().item()\n",
    "    pred_max = pred_ext.max().item()\n",
    "    has_nan = torch.isnan(pred_ext).any().item()\n",
    "    has_inf = torch.isinf(pred_ext).any().item()\n",
    "    \n",
    "    initial_scale = pred_ext[:, :T].abs().mean().item() + 1e-6\n",
    "    final_scale = pred_ext[:, -T:].abs().mean().item()\n",
    "    ratio = final_scale / initial_scale\n",
    "\n",
    "print(f'  Rollout: 90 days')\n",
    "print(f'  Range: [{pred_min:.4f}, {pred_max:.4f}]')\n",
    "print(f'  Scale ratio: {ratio:.2f}x')\n",
    "print(f'  NaN/Inf: {has_nan or has_inf}')\n",
    "\n",
    "test2_pass = not (has_nan or has_inf) and ratio < 100 and pred_max < 1000\n",
    "print(f'  Result: {\"[PASS]\" if test2_pass else \"[FAIL]\"} System is {\"stable\" if test2_pass else \"unstable\"}')\n",
    "\n",
    "# =============================================================================\n",
    "# TEST 3: Graphon Generalization\n",
    "# =============================================================================\n",
    "print('\\n[TEST 3] Graphon Generalization (Scale Invariance)')\n",
    "print('-' * 40)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.network.reset_cache()\n",
    "    \n",
    "    x = sample_batch['x']\n",
    "    edge_index = sample_batch['edge_index']\n",
    "    \n",
    "    pred_n, _ = model.network(x, edge_index)\n",
    "    mean_n = pred_n.abs().mean().item()\n",
    "    \n",
    "    # Double nodes\n",
    "    B, T, N, F = x.shape\n",
    "    x_2n = x.repeat(1, 1, 2, 1)\n",
    "    edge_2n = torch.cat([edge_index, edge_index + N], dim=1)\n",
    "    \n",
    "    model.network.reset_cache()\n",
    "    pred_2n, _ = model.network(x_2n, edge_2n)\n",
    "    mean_2n = pred_2n.abs().mean().item()\n",
    "    \n",
    "    deviation = abs(mean_2n - mean_n) / (mean_n + 1e-8)\n",
    "\n",
    "print(f'  N={N} mean: {mean_n:.6f}')\n",
    "print(f'  N={2*N} mean: {mean_2n:.6f}')\n",
    "print(f'  Deviation: {100*deviation:.2f}%')\n",
    "\n",
    "test3_pass = deviation < 0.10\n",
    "print(f'  Result: {\"[PASS]\" if test3_pass else \"[FAIL]\"} Scale invariance {\"within\" if test3_pass else \"exceeds\"} 10% tolerance')\n",
    "\n",
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "print('\\n' + '='*60)\n",
    "print('AUDIT SUMMARY')\n",
    "print('='*60)\n",
    "print(f'  {\"[x]\" if test1_pass else \"[ ]\"} Counterfactual (Temperature): {\"PASS\" if test1_pass else \"FAIL\"}')\n",
    "print(f'  {\"[x]\" if test2_pass else \"[ ]\"} Long-Horizon Stability: {\"PASS\" if test2_pass else \"FAIL\"}')\n",
    "print(f'  {\"[x]\" if test3_pass else \"[ ]\"} Graphon Generalization: {\"PASS\" if test3_pass else \"FAIL\"}')\n",
    "\n",
    "all_pass = test1_pass and test2_pass and test3_pass\n",
    "print(f'\\n{\"âœ… All tests PASSED\" if all_pass else \"âŒ Some tests FAILED\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/GLKAN_Project/outputs'\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "results = {\n",
    "    'timestamp': timestamp,\n",
    "    'config': CONFIG,\n",
    "    'training': {\n",
    "        'epochs': len(history['train_loss']),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'final_train_loss': history['train_loss'][-1] if history['train_loss'] else None,\n",
    "        'final_val_loss': history['val_loss'][-1] if history['val_loss'] else None,\n",
    "    },\n",
    "    'regression': {\n",
    "        'rmse': float(rmse),\n",
    "        'mae': float(mae),\n",
    "        'pred_std': float(pred_valid.std()),\n",
    "    },\n",
    "    'scientific_audit': {\n",
    "        'counterfactual': test1_pass,\n",
    "        'long_horizon': test2_pass,\n",
    "        'graphon': test3_pass,\n",
    "        'all_passed': all_pass,\n",
    "    },\n",
    "    'history': history,\n",
    "}\n",
    "\n",
    "results_path = f'{OUTPUT_DIR}/results_{timestamp}.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f'\\nâœ… Results saved to: {results_path}')\n",
    "print(f'Checkpoints at: {CHECKPOINT_DIR}')\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Loss\naxes[0, 0].plot(history['train_loss'], label='Train Loss', color='blue', alpha=0.8)\naxes[0, 0].plot(history['val_loss'], label='Val Loss', color='orange', alpha=0.8)\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].set_title('Training & Validation Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].set_yscale('log')\n\n# RMSE\naxes[0, 1].plot(history['train_rmse'], label='Train RMSE', color='blue', alpha=0.8)\naxes[0, 1].plot(history['val_rmse'], label='Val RMSE', color='orange', alpha=0.8)\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('RMSE')\naxes[0, 1].set_title('Training & Validation RMSE')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Learning Rate\naxes[1, 0].plot(history['lr'], label='Learning Rate', color='green', alpha=0.8)\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Learning Rate')\naxes[1, 0].set_title('Learning Rate Schedule')\naxes[1, 0].set_yscale('log')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Loss components (if available)\naxes[1, 1].plot(history['train_loss'], label='Train Loss', color='blue', alpha=0.8)\nbest_epoch = np.argmin(history['val_loss'])\naxes[1, 1].axvline(x=best_epoch, color='red', linestyle='--', label=f'Best (epoch {best_epoch+1})')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('Loss')\naxes[1, 1].set_title(f'Best Model @ Epoch {best_epoch+1}')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(f'{OUTPUT_DIR}/training_curves_{timestamp}.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f'\\nâœ… Training curves saved to: {OUTPUT_DIR}/training_curves_{timestamp}.png')\n\n# Print training summary\nprint(f'\\n{\"=\"*60}')\nprint('TRAINING SUMMARY')\nprint(f'{\"=\"*60}')\nprint(f'  Total epochs: {len(history[\"train_loss\"])}')\nprint(f'  Best epoch: {best_epoch+1}')\nprint(f'  Best val loss: {min(history[\"val_loss\"]):.6f}')\nprint(f'  Final train loss: {history[\"train_loss\"][-1]:.6f}')\nprint(f'  Final val loss: {history[\"val_loss\"][-1]:.6f}')\nprint(f'  Final val RMSE: {history[\"val_rmse\"][-1]:.4f}')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}