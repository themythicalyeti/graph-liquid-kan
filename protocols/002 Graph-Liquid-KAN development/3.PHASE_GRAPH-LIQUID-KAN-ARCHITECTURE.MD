# Protocol for Phase 3: The Graph-Liquid-KAN Architectural Synthesis

**Objective:**
Construct the **Graph-Liquid-KAN (GL-KAN)** neural processing unit. This phase shifts from data engineering to precision software architecture. You will build a custom PyTorch module that fuses three distinct mathematical concepts: **Graphon Topology**, **Liquid Time-Constant Dynamics**, and **Kolmogorov-Arnold Functional Approximation**.

**Constraint:**
Do not use standard "black box" layers like `nn.Linear` or `nn.LSTM` inside the core dynamics. Every transformation must be explicit and mathematically justifiable to ensure the "Glass Box" interpretability required by the synthesis.

**Warning:**
The interaction between the **Sparse Topology** (Adjacency Matrix) and the **Dense KAN Grids** (RBF Expansion) is the most likely point of failure. If you apply the KAN expansion *before* aggregation on the wrong axis, you will blow up video memory (VRAM). Follow the order of operations strictly.

---

## Sub-Module 3.1: The Atomic FastKAN Layer

**Theory:**
Standard Multi-Layer Perceptrons (MLPs) multiply inputs by a weight matrix and then apply a fixed non-linearity (ReLU). KANs reverse this: they apply a learnable non-linearity (a spline or basis function) to *every* edge, then sum them. To make this computationally feasible for our large graph, we use the **FastKAN** approximation, which uses Gaussian Radial Basis Functions (RBFs).

**Protocol:**

1. **Define the Grid:**
Create a non-learnable parameter tensor representing the centers of your Gaussian functions. These should be evenly spaced between -1 and 1. This defines the "resolution" of the function the network can learn.
2. **Input Normalization (Critical Safety Interlock):**
Before any data enters the KAN layer, you must apply **Layer Normalization**.
* *Reasoning:* RBFs have a local support (they are only active near their center). If your input data drifts to values like 5.0 or 10.0, it will fall outside the grid range [-1, 1]. The gradient will be zero, the neuron will "die," and the model will fail to learn. LayerNorm forces the data back into the active range of the grid.


3. **The Basis Expansion (The "Memory Hazard"):**
You must expand the input tensor  of shape `into a higher-dimensional tensor of shape`.
* For every feature, calculate its distance to *every* grid center.
* Apply the Gaussian function: .
* *Do not* simply loop. Use array broadcasting to perform this in a single vectorized operation.


4. **The Weight Aggregation:**
Perform a linear combination (weighted sum) of these basis functions. This transforms the shape `back down to`.
* *Note:* This effectively learns the "shape" of the curve connecting input to output.



---

## Sub-Module 3.2: The Graphon-Compliant Aggregator

**Theory:**
We are modeling a continuous physical field (the ocean), not just a social network. Therefore, as the number of farms () increases, the total "signal" received by a node should not explode. It must converge to a mean-field integral.

**Protocol:**

1. **Define the Normalization Factor:**
In standard GCNs, you normalize by the square root of the degree. In Graphon-NDEs (Neural Differential Equations), you must normalize by **** (the total number of nodes) or **** (the local density).
* *Decision:* Use  (Row Normalization) for the adjacency matrix. This ensures that a farm surrounded by 50 neighbors does not receive 50x the infection pressure of a farm with 1 neighbor, but rather an *average* infection pressure, which aligns with physical concentration laws.


2. **Sparse Matrix Multiplication:**
Use PyTorch's sparse matrix multiplication (`torch.sparse.mm`).
* *Input:* The sparse Adjacency Matrix () and the Dense Node States ().
* *Operation:* .
* *Output:* A dense tensor  representing the "Ambient Infection Pressure" at each farm location.



---

## Sub-Module 3.3: The Liquid-KAN Cell (The Core Engine)

**Theory:**
This is the heart of the architecture. We replace the standard Recurrent Neural Network (RNN) update rule with a **Closed-form Continuous (CfC)** update rule, parameterized by KANs.

**Protocol for the "Forward" Method:**

1. **Inputs:**
* Current Hidden State ()
* Current Environmental Input () (Temperature, Salinity, Currents)
* Current Ambient Infection Pressure () (Result from Sub-Module 3.2)
* Time Step ()


2. **Construct the Context Vector:**
Concatenate  and . This vector represents the total "forcing" acting on the biological system at this moment.
3. **Calculate the "Liquid" Time-Constant ():**
Pass the Context Vector through a **FastKAN** layer (from Sub-Module 3.1).
* *Constraint:* Time constants must be positive.
* *Transformation:* Apply a **Softplus** function to the KAN output, then add a small epsilon (e.g., 0.01).
* *Physics:* This learns how fast the lice population reacts. Example: High Temperature -> Low  (Fast growth). Low Temperature -> High  (Slow/Frozen dynamics).


4. **Calculate the Equilibrium Target:**
Pass the Context Vector through a *second, separate* **FastKAN** layer.
* *Physics:* This represents the "Carrying Capacity" or the steady-state lice level given the current environment if time were infinite.


5. **The Exact Integration Step (CfC):**
Do not use Euler integration (). Use the closed-form solution to the differential equation:
* Calculate the **Decay Factor**: .
* Update State: .
* *Result:* This update step is numerically stable even if  is large or irregular.



---

## Sub-Module 3.4: The Temporal Unroller (Network Assembly)

**Theory:**
We must wrap the Cell into a network that can process the sequence data we prepared in Phase 2.

**Protocol:**

1. **Initialization:**
Define the storage for the trajectory (all history states). Initialize the hidden state  to zeros (or a learnable parameter).
2. **The Time Loop:**
Iterate through the time dimension  of your input tensor.
* **Step A:** Extract the slice  for the current day.
* **Step B (Spatial Mixing):** Run the Graphon Aggregator (Sub-Module 3.2) using the *previous* state  to get Infection Pressure .
* **Step C (Temporal Evolution):** Run the Liquid-KAN Cell (Sub-Module 3.3) using  to get .
* **Step D:** Store .


3. **The Projection Head:**
After the loop, you have a sequence of hidden states. Pass these through a final **FastKAN** layer to project them to the target dimension (1 dimension: Lice Count).
* *Output Shape:* ``.



---

## Validation Checkpoints

Before moving to training, you must verify the architecture via "Dry Runs":

1. **Gradient Flow Check:**
Pass a dummy batch through the network. Call `.backward()`. Check the gradients of the RBF grid centers.
* *Failure Condition:* If gradients are all zero, your input data was not normalized (LayerNorm failed), and the RBFs are "dead."


2. **Time-Constant Inspection:**
Pass a batch of data with varying temperatures. Extract the generated  values.
* *Success Condition:*  should vary. If  is constant for all inputs, the KAN layer acting as the "Liquid" controller is broken or initialized with weights that are too small.


3. **Graphon Stability Test:**
Run the model with a graph of size  and a graph of size  (using duplicated data).
* *Success Condition:* The average magnitude of the activations should remain roughly constant. If the  output is 10x larger, your Aggregation Normalization () is missing.



**Action Item:**
You are now ready to write the code for `layers.py` (FastKAN and GraphonAggregator) and `models.py` (LiquidKANCell and GLKAN_Network). Do not start training until these dry runs pass.