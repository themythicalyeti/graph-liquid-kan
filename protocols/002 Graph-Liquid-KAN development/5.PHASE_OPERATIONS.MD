# Protocol for Phase 5: Operational Verification, Inference Architecture, and Deployment

**Objective:**
Transition the **Graph-Liquid-KAN (GL-KAN)** from a trained experimental model to a deployable **Digital Twin** capable of real-time forecasting and counterfactual scenario analysis. This phase addresses the "Black Box" problem by quantifying the stability of the learned dynamics and establishing a rigid pipeline for inference on live data streams.

**Critical Constraint:**
In a production environment, future ground-truth lice counts () are unavailable. The model must operate in **Autoregressive Mode** or **Open-Loop Mode** using only environmental forecasts () and its own internal memory (). You must implement a "State Persistence Manager" to maintain the hidden states of 1,000+ farms between daily API calls.

---

## Sub-Module 5.1: The Stability Audit (Lyapunov Analysis)

**Theory:**
A Neural ODE can achieve low training error while learning unstable, chaotic dynamics that explode when projected into the future. To prove the model is safe for biological regulation, we must calculate the **Maximal Lyapunov Exponent (MLE)**. If , the system is chaotic. If , the system is stable (desired for regulated biological populations).

**Protocol:**

1. **Jacobian Extraction:**
* For a batch of validation data, perform a forward pass through the **Liquid-KAN Cell** (defined in Phase 3).
* Instead of the loss, compute the **Jacobian Matrix**  of the update step. This measures how a small perturbation in the state vector expands or contracts over one time step.
* *Engineering Note:* Use `torch.autograd.functional.jacobian` for exact computation. Do not approximate.


2. **Lyapunov Exponent Calculation:**
* Initialize a random perturbation vector  of unit length.
* Iterate through the time sequence: .
* Normalize  at every step to prevent numerical overflow, accumulating the log of the norms: .
* The MLE estimate is .


3. **Pass/Fail Criteria:**
* If  for more than 5% of the farms, the model has learned a chaotic attractor. You must retrain with a higher penalty coefficient  (from Phase 4) or enforce a stricter spectral norm constraint on the recurrent weights.



---

## Sub-Module 5.2: The Digital Twin Interface (Counterfactuals)

**Theory:**
The primary value of this system is simulating "What If" scenarios (e.g., "If we treat Farm A on Tuesday, how does that affect Farm B next week?"). This requires injecting **Control Signals** into the graph.

**Protocol:**

1. **Augment the Feature Space:**
* Modify the input tensor  to include a "Treatment Channel" .
* During training (Phase 4), this channel was populated with historical records.
* During inference, this channel becomes a user-controllable variable.


2. **The Simulator Class:**
* Create a class `SeaLiceSimulator`.
* **Input:** Initial State  (from live production), Weather Forecast , and a Treatment Plan  (sparse tensor of scheduled treatments).
* **Execution:** Run the GL-KAN forward in **Closed-Loop Mode**.
* **Constraint:** Treatments do not instantaneously remove lice in the ODE; they alter the mortality rate. Ensure the `LeakKAN` (Phase 3) receives  as an input so it can learn to spike the decay rate  when treatment occurs.



---

## Sub-Module 5.3: The Rolling Inference Engine

**Theory:**
In production, data arrives asynchronously. NorKyst updates daily; Barentswatch updates weekly. The system must maintain a "Live State"  that is updated whenever new data arrives.

**Protocol:**

1. **State Persistence Database:**
* Do not store the hidden state  in RAM (it will be lost on restart).
* Implement a **Tensor IO** layer that serializes the hidden state  to disk (e.g., using `safetensors` or a quantized HDF5 format) after every daily update.


2. **The "Coasting" Algorithm (Handling Missing Data):**
* **Scenario:** It is Tuesday. You have ocean data () but no lice count ().
* **Action:** Run the GL-KAN cell to compute  using  and the previous .
* **Correction:** When the official lice count  finally arrives (e.g., next Monday), you must perform a **State Correction** (Data Assimilation).
* Standard approach: Reset  to encode .
* GL-KAN approach: Use an **Ensemble Kalman Filter (EnKF)** or simply re-run the model from the last known ground truth (Monday) through to today (Tuesday) using the recorded  data. This "Replay" strategy is robust and ensures the hidden state remains consistent with the ODE dynamics.





---

## Sub-Module 5.4: Deployment Compilation (Freezing the KANs)

**Theory:**
Kolmogorov-Arnold Networks are computationally heavy due to spline/RBF evaluation. For deployment, we must strip away the training overhead (dynamic grid adaptation) and compile the network into a static computational graph.

**Protocol:**

1. **Grid Freezing:**
* Before export, loop through all `FastKANLayer` modules.
* Set `requires_grad=False` on all parameters.
* Merge the `LayerNorm` statistics (mean/std) into the input projection if possible, or ensure the normalization layer is rigidly defined in the export.


2. **JIT Compilation (TorchScript):**
* Use `torch.jit.script` (preferable) or `torch.jit.trace`.
* *Hazard:* KANs rely on advanced tensor indexing (broadcasting) that simpler ONNX exporters might misinterpret.
* *Solution:* Explicitly define the RBF expansion as a standalone TorchScript function:
```python
@torch.jit.script
def rbf_expansion(x: Tensor, grid: Tensor) -> Tensor:
    #... optimized explicit expansion code...
    return basis

```


* Replace the dynamic Python method in your `FastKANLayer` with this JIT-compiled function before exporting the full model.


3. **The Deployment Container:**
* Package the model into a Docker container containing:
* The JIT-compiled `model.pt`.
* The `State Persistence Manager` (Sub-module 5.3).
* A lightweight API server (FastAPI) that accepts JSON inputs (Farm IDs, Date) and returns JSON predictions.





---

## Execution Checklist for Phase 5

1. **Implement `audit.py**`: Calculate Lyapunov exponents. If unstable, go back to Phase 4 and increase .
2. **Implement `simulator.py**`: The "What-If" engine allowing treatment injection.
3. **Implement `production.py**`: The rolling state updater with disk persistence and "Replay" correction logic.
4. **Run `export_model.py**`: Verify that the JIT-compiled model produces *identical* outputs to the Python model (tolerance ).

**Final Deliverable:**
A fully autonomous software system that ingests raw data, updates a biological Digital Twin, and serves predictions via API, validated by rigorous mathematical stability metrics. You are now ready to write the final code.