# Protocol for Phase 4: The Physics-Informed Optimization and Training Regime

**Objective:**
Operationalize the Graph-Liquid-KAN (GL-KAN) architecture constructed in Phase 3. This phase defines the rigorous training loop, the physics-informed loss landscapes, and the dynamic grid adaptation mechanisms required to stabilize the Kolmogorov-Arnold Networks during gradient descent.

**Critical Constraint:**
You are training a **Hybrid Dynamic System**, not a static classifier. Standard training loops will fail.

1. **KAN Grid Collapse:** If the input feature distribution shifts during training, the KAN's fixed grid will lose support, resulting in zero gradients. You **must** implement Dynamic Grid Adaptation.
2. **Exploding Time-Constants:** The "Liquid" parameter  can drift towards zero (singularity) or infinity (freezing). You **must** implement barrier penalties.
3. **Physical Violation:** The model may achieve low MSE by predicting biologically impossible negative lice counts or infinite growth. You **must** implement Physics-Informed Regularization.

---

## Sub-Module 4.1: The Physics-Informed Loss "Legislator"

**Theory:**
We do not minimize error; we minimize **Energy**. The loss function must represent the deviation from observed data *plus* the deviation from physical and biological laws.

**Protocol:**

1. **Define the Data Fidelity Term ():**
* Use **Huber Loss** (SmoothL1Loss) instead of MSE.
* *Reasoning:* Biological data contains outliers (measurement errors). MSE squares these errors, causing massive gradient spikes that destabilize the ODE solver. Huber loss is quadratic near zero but linear for large errors, providing robustness.
* *Masking:* You must multiply the loss by the `mask` tensor created in Phase 2. We only compute loss on days where ground-truth observations exist (Mondays). The ODE fills in the gaps, but we do not penalize the gaps.


2. **Define the Biological Constraint Term ():**
* **Non-Negativity Constraint:** Calculate `ReLU(-prediction)`. If the model predicts negative lice, this term becomes positive. Minimize it to zero.
* **Growth Rate Bound:** The derivative  cannot exceed the biological limit of lice reproduction.
* Calculate the finite difference: .
* Define a maximum change threshold  (derived from literature, e.g., 20% per day).
* Constraint: `ReLU(|\Delta H| - \delta_{max})`.




3. **Define the System Stability Term ():**
* **Time-Constant Regularization:** The "Liquid" time constant  must not be too small (stiffness) or too large (irresponsive).
* *Penalty:* . This convex "hook" function forces  toward 1.0, penalizing extremes.


4. **Total Loss Aggregation:**
* .
* *Initial Configuration:* Set  and .



---

## Sub-Module 4.2: The KAN Grid Adaptation Mechanism

**Theory:**
FastKAN uses Gaussian RBFs centered on a grid (e.g., -2 to +2). If the node embeddings drift to +5.0 during training, they fall into the "dead zone" of the RBFs. The model stops learning. We must dynamically move the grid to chase the data.

**Protocol:**

1. **The Trigger Condition:**
* Do not update every step. Update the grid every  epochs (e.g., ).


2. **The Adaptation Routine:**
* **Step A: Collection.** Run a forward pass on a representative subset of the training data (e.g., one batch). Do *not* calculate gradients.
* **Step B: Profiling.** Record the minimum and maximum activation values of the *input* to every KAN layer.
* **Step C: Realignment.**
* If the data range  exceeds the current grid range, re-initialize the grid parameter `self.grid`.
* *Crucial Step:* You must use the `state_dict` to copy the learned spline coefficients to the new grid positions via interpolation. If you simply reset the grid, you erase the learned memory.
* *Simplification for Phase 4:* For the first iteration, simply **clamp** the inputs to the fixed grid range using Tanh or LayerNorm (implemented in Phase 3). True dynamic grid adaptation is complex; reliable input normalization is the robust engineering solution. **Enforce LayerNorm at all KAN inputs.**





---

## Sub-Module 4.3: The Training Executive (The Loop)

**Theory:**
We use a standard Backpropagation Through Time (BPTT) approach, but we must carefully manage the hidden state to simulate continuous time across batched windows.

**Protocol:**

1. **Optimizer Configuration:**
* Use **AdamW**. KANs require weight decay to prevent the spline coefficients from oscillating wildly.
* *Learning Rate:* Start small (). KANs are sensitive.
* *Scheduler:* Use `ReduceLROnPlateau`. If the validation loss stagnates, drop the LR by factor 0.5.


2. **The Epoch Loop:**
* **Input:** The `DataLoader` from Phase 2 (providing `X` window, `Y` window, `Mask`, `EdgeIndex`).
* **Forward Pass:**
* Initialize .
* Unroll the network over the time window .
* Collect predictions  and time-constants  for every step.


* **Loss Calculation:**
* Compute  using the logic from Sub-Module 4.1.


* **Backward Pass:**
* `loss.backward()`
* **Gradient Clipping:** Apply `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)`. This is non-negotiable for ODE/RNN stability.
* `optimizer.step()`
* `optimizer.zero_grad()`




3. **The Teacher Forcing Schedule:**
* At Epoch 0, feed the *Ground Truth* lice counts from the previous day as input to the next step (if available).
* Linearly decay this probability to 0.0 over 50 epochs. This forces the model to learn to rely on its own internal ODE dynamics rather than just copying the previous day's value.



---

## Sub-Module 4.4: The Scientific Validation "Auditor"

**Theory:**
Low loss is not enough. We must validate that the model has learned the *physics* of the system.

**Protocol:**

1. **The Counterfactual Test:**
* Take a validation batch. Artificially increase the Temperature channel by +5Â°C.
* Run the model.
* *Check:* Does the lice growth rate increase?
* *Pass Criteria:* If growth decreases or stays flat, the model has failed to learn the biological law.


2. **The Long-Horizon Rollout:**
* Feed the model initial conditions ().
* Run it for 90 days (3x the training window) without any new lice data (only ocean data).
* *Check:* Does the prediction explode to infinity? Does it crash to zero? Or does it oscillate stably?
* *Pass Criteria:* The trajectory must remain bounded (Input-to-State Stability).


3. **The Graphon Generalization Test:**
* Create a synthetic graph with 2x the nodes (duplicate existing nodes).
* Run the model.
* *Check:* Is the average predicted lice count roughly the same as the original graph?
* *Pass Criteria:* Deviation < 10%. If deviation is high, the Graphon Normalization () in Phase 3 was implemented incorrectly.



---

## Execution Checklist for Phase 4

1. **Implement** `losses.py`: The custom loss class with Huber, Biological, and Stability terms.
2. **Implement** `train.py`: The loop with Gradient Clipping and Teacher Forcing.
3. **Implement** `audit.py`: The counterfactual and stability tests.
4. **Run** a "Overfit on One Batch" test. The model should reach Loss ~ 0.0 on a single farm's data. If not, the architecture is broken.
5. **Run** full training. Monitor the `Tau_Histogram` in Tensorboard.

**Deliverable:**
A trained `model_weights.pt` file and a `validation_report.pdf` confirming the stability tests passed. You are now ready to code the training logic.