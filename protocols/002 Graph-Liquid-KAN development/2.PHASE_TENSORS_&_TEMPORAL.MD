# Protocol for Phase 2: Tensor Construction and Temporal Alignment

**Objective:** Transform the raw geospatial artifacts (CSVs and NetCDF files) from Phase 1 into a rigorous `GraphTemporalSignal` compatible with PyTorch Geometric. This phase bridges the gap between discrete biological observations (weekly lice counts) and continuous physical forcing (hourly ocean data).

**Critical Constraint:** The **Graph-Liquid-KAN** architecture requires a specific tensor structure to function. The "Liquid" time-constant  depends on the *continuity* of the input signal. Discontinuities or misalignment between the ocean forcing () and the biological response () will cause the ODE solver to diverge. You must follow the alignment sub-protocols strictly.

---

## Sub-Module 2.1: The Biological Label Acquisition

**Status:** In Phase 1, we defined the *Nodes* (Farm Locations). Now we must populate these nodes with their historical *State* (Lice Counts).

**The Data Source:** Barentswatch FishHealth API (`/v1/geodata/fishhealth/locality/{localityId}/{year}/{week}`).
**Latency Warning:** Fetching 10 years of history for ~1000 farms requires  API calls. This cannot be done synchronously without hitting timeouts.

### Step 1: The Asynchronous Harvester

You must implement a script that iterates through the `graph_nodes_metadata.csv` produced in Phase 1.

**Protocol:**

1. **Load** `graph_nodes_metadata.csv`.
2. **Iterate** through each `localityNo`.
3. **Request** the "Locality History" from Barentswatch for years 2012â€“2024.
4. **Extract** the specific metric: `avgAdultFemaleLice`. If missing, mark as `NaN` (do not impute yet; the ODE will handle missingness).
5. **Structure** the output as a row-oriented CSV: `locality_no, year, week, lice_count`.

**Create File:** `data_gatherers/fetch_lice_history.py`

```python
import pandas as pd
import time
from bw_client import BarentsWatchClient # From Phase 1
import os

def fetch_history():
    client = BarentsWatchClient()
    nodes = pd.read_csv("graph_nodes_metadata.csv")
    
    # Check for existing checkpoint to resume if crashed
    if os.path.exists("lice_history_raw.csv"):
        existing = pd.read_csv("lice_history_raw.csv")
        processed_ids = set(existing['localityNo'].unique())
    else:
        processed_ids = set()
        # Write header
        with open("lice_history_raw.csv", "w") as f:
            f.write("localityNo,year,week,lice_count\n")

    print(f"Starting harvest for {len(nodes)} farms...")
    
    for idx, row in nodes.iterrows():
        loc_id = row['localityNo']
        if loc_id in processed_ids:
            continue
            
        print(f"Fetching history for Farm {loc_id}...")
        all_records =
        
        # Loop years. Barentswatch often requires per-year queries or returns full history.
        # We assume the endpoint returns full history for efficiency if available, 
        # otherwise loop years 2015-2024.
        for year in range(2015, 2025):
            try:
                # Endpoint: /v1/geodata/fishhealth/locality/{localityId}/{year}
                # Note: Verify exact endpoint in documentation as it changes.
                data = client.get_request(f"/geodata/fishhealth/locality/{loc_id}/{year}")
                
                if 'fishHealthData' in data:
                    for week_data in data:
                        # We specifically need Adult Female Lice for legislation compliance
                        # Handle cases where value is None
                        lice = week_data.get('avgAdultFemaleLice', None)
                        week = week_data.get('week')
                        all_records.append(f"{loc_id},{year},{week},{lice if lice is not None else ''}")
            except Exception as e:
                print(f"Failed for {loc_id} year {year}: {e}")
                time.sleep(1) # Cool down
        
        # Flush to disk immediately to prevent data loss
        with open("lice_history_raw.csv", "a") as f:
            for line in all_records:
                f.write(line + "\n")
        
        time.sleep(0.1) # Rate limit courtesy

if __name__ == "__main__":
    fetch_history()

```

---

## Sub-Module 2.2: The Temporal Alignment Engine

**Objective:** We now have two disparate time-series:

1. **Ocean:** Hourly/Daily resolution, indexed by Date.
2. **Lice:** Weekly resolution, indexed by ISO Week Number.

We must merge these into a single Tensor .

**Decision:** The "Liquid" ODE solver works best with continuous signals. We will **Upsample** the Lice data to match the Ocean data resolution (Daily), but we will add a **Mask Channel** indicating which days have ground-truth observations. The ODE will learn to interpolate the dynamics between these truth points.

### Step 1: The `DatasetBuilder` Class

This script creates the final `.pt` (PyTorch) files.

**Logic Flow:**

1. Define a master timeline (e.g., `2023-01-01` to `2023-12-31` daily).
2. **Ocean Processing:**
* Load `hydrography_2023.nc`.
* Resample from Hourly to Daily Average (to reduce sequence length for initial training).
* Extract features: `temperature`, `salinity`, `current_speed` ().
* Normalize: . **Crucial:** Calculate  over the training period only.


3. **Lice Processing:**
* Map `(Year, Week)` to `Date` (Monday of that week).
* Create a sparse tensor where values exist only on Mondays.
* Create a boolean mask tensor (`True` on Mondays, `False` otherwise).


4. **Save:** Serialize as a dictionary of tensors.

**Create File:** `data_processors/builder.py`

```python
import xarray as xr
import pandas as pd
import numpy as np
import torch
import os

class GLKANDatasetBuilder:
    def __init__(self, raw_lice_path, hydro_path, nodes_path):
        self.lice_df = pd.read_csv(raw_lice_path)
        self.hydro_ds = xr.open_dataset(hydro_path) # Lazy load
        self.nodes = pd.read_csv(nodes_path)
        self.farm_ids = self.nodes['localityNo'].values
        
        # Create map from FarmID -> Array Index
        self.id_to_idx = {id: i for i, id in enumerate(self.farm_ids)}
        
    def process_year(self, year):
        print(f"Processing year {year}...")
        
        # 1. Define Temporal Grid (Daily)
        start_date = f"{year}-01-01"
        end_date = f"{year}-12-31"
        dates = pd.date_range(start=start_date, end=end_date, freq='D')
        T = len(dates)
        N = len(self.farm_ids)
        
        # 2. Process Hydrography (Features)
        # Calculate Magnitude of current
        u = self.hydro_ds['u_eastward'].sel(time=slice(start_date, end_date))
        v = self.hydro_ds['v_northward'].sel(time=slice(start_date, end_date))
        temp = self.hydro_ds['temperature'].sel(time=slice(start_date, end_date))
        salt = self.hydro_ds['salinity'].sel(time=slice(start_date, end_date))
        
        # Resample to Daily Mean (coarsen)
        # Note: If memory is tight, do this in chunks.
        u_daily = u.resample(time='1D').mean().values # Shape (T, N)
        v_daily = v.resample(time='1D').mean().values
        temp_daily = temp.resample(time='1D').mean().values
        salt_daily = salt.resample(time='1D').mean().values
        
        # Feature Tensor: (T, N, F)
        # Features:
        # We use Sin/Cos for direction to avoid discontinuity at 360 degrees
        current_speed = np.sqrt(u_daily**2 + v_daily**2)
        current_angle = np.arctan2(v_daily, u_daily)
        
        X = np.stack([
            temp_daily, 
            salt_daily, 
            current_speed, 
            np.sin(current_angle), 
            np.cos(current_angle)
        ], axis=-1)
        
        # Handle NaNs (Land points or model errors) -> Zero fill or interpolate
        # For Graph-Liquid-KAN, explicit NaNs will crash gradients. 
        # We fill with local mean or 0 (if normalized).
        X = np.nan_to_num(X)

        # 3. Process Lice (Labels)
        Y = np.zeros((T, N))
        Y_mask = np.zeros((T, N))
        
        # Filter lice data for this year
        year_lice = self.lice_df[self.lice_df['year'] == year]
        
        for _, row in year_lice.iterrows():
            fid = row['localityNo']
            if fid not in self.id_to_idx: continue
            
            f_idx = self.id_to_idx[fid]
            week = int(row['week'])
            val = row['lice_count']
            
            if pd.isna(val): continue
            
            # Convert Week to Date (Monday)
            # using '1' for Monday
            try:
                date_str = f"{year}-W{week:02d}-1"
                obs_date = datetime.strptime(date_str, "%Y-W%W-%w")
                
                # Find index in our timeline
                time_idx = (obs_date - pd.Timestamp(start_date).to_pydatetime()).days
                
                if 0 <= time_idx < T:
                    Y[time_idx, f_idx] = float(val)
                    Y_mask[time_idx, f_idx] = 1.0
            except ValueError:
                pass # Week 53 issues or bad formatting

        return {
            'X': torch.FloatTensor(X),       # (T, N, Features)
            'Y': torch.FloatTensor(Y),       # (T, N)
            'mask': torch.BoolTensor(Y_mask),# (T, N)
            'farm_ids': torch.LongTensor(self.farm_ids),
            'dates': dates
        }

if __name__ == "__main__":
    # Ensure you ran Phase 1 scripts first!
    builder = GLKANDatasetBuilder(
        "lice_history_raw.csv", 
        "hydrography_2023.nc", 
        "graph_nodes_with_grid_indices.csv"
    )
    data = builder.process_year(2023)
    torch.save(data, "dataset_2023.pt")
    print("Dataset built and saved.")

```

---

## Sub-Module 2.3: The Topological Constructor

**Status:** We have node features. Now we need the **Adjacency Matrix** ().
For the Graph-Liquid-KAN to utilize the **Graphon 1/N scaling**, we must carefully define the edges.

**Constraint:** The graph should not be fully connected (too computationally expensive, ). We will use a **Distance-Based Threshold** (Geometric Graph). Sea lice larvae drift ~30km on average. We should create edges between farms within hydrodynamic interaction range.

**Create File:** `data_processors/graph_builder.py`

```python
import torch
import pandas as pd
import numpy as np
from scipy.spatial.distance import cdist

def build_topology(nodes_path, distance_threshold_km=30.0):
    nodes = pd.read_csv(nodes_path)
    
    # Extract Lat/Lon
    # Formula: Haversine distance
    coords = nodes[['lat', 'lon']].values
    
    # Convert to radians
    coords_rad = np.radians(coords)
    
    # Earth radius
    R = 6371.0 
    
    print("Computing distance matrix...")
    # cdist is efficient for N=1000
    # Custom haversine metric:
    # 2 * R * arcsin(sqrt(sin^2(dlat/2) + cos(lat1)cos(lat2)sin^2(dlon/2)))
    # For speed, we approximate using Euclidean on scaled lat/lon or use libraries.
    # Here is a robust approximation for short distances:
    
    dlat = coords_rad[:, 0, None] - coords_rad[:, 0]
    dlon = coords_rad[:, 1, None] - coords_rad[:, 1]
    
    a = np.sin(dlat/2)**2 + np.cos(coords_rad[:, 0, None]) * np.cos(coords_rad[:, 0]) * np.sin(dlon/2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))
    dist_matrix = R * c
    
    # Thresholding
    # Create adjacency: 1 if dist < threshold, 0 else
    # We exclude self-loops for the message passing (optional, but standard)
    adj = (dist_matrix < distance_threshold_km).astype(np.float32)
    np.fill_diagonal(adj, 0)
    
    # Convert to COO format (Edge Index) for PyTorch Geometric
    src, dst = np.where(adj > 0)
    edge_index = torch.tensor([src, dst], dtype=torch.long)
    
    print(f"Graph constructed. Nodes: {len(nodes)}, Edges: {edge_index.shape[1]}")
    
    # Graphon Normalization Pre-calculation
    # Compute degree for each node
    degree = torch.tensor(adj.sum(axis=1))
    
    return edge_index, degree

if __name__ == "__main__":
    edge_index, degree = build_topology("graph_nodes_with_grid_indices.csv")
    torch.save({'edge_index': edge_index, 'degree': degree}, "spatial_graph.pt")

```

---

## Sub-Module 2.4: The PyG Dataset Class

**The Final Assembly.** This class defines how the neural network sees the data. It extends `torch_geometric.data.Dataset`.

**Create File:** `dataset.py`

```python
import torch
from torch_geometric.data import Dataset, Data

class SeaLiceGraphDataset(Dataset):
    def __init__(self, feature_file, graph_file, window_size=30):
        super().__init__()
        # Load the pre-processed tensors
        self.features = torch.load(feature_file) # dict: X, Y, mask
        self.graph = torch.load(graph_file)      # dict: edge_index, degree
        
        self.window_size = window_size
        self.num_nodes = self.features['X'].shape[1]
        self.total_days = self.features['X'].shape
        
    def len(self):
        # We slide a window over the time series
        # Example: Days 0-30 predict Day 31 (or just model dynamics 0-30)
        return self.total_days - self.window_size

    def get(self, idx):
        # Slice the temporal window
        # Shape: (Window, Nodes, Features)
        x_window = self.features['X'][idx : idx + self.window_size]
        y_window = self.features[idx : idx + self.window_size]
        mask_window = self.features['mask'][idx : idx + self.window_size]
        
        # Construct PyG Data Object
        # Note: PyG usually expects (Nodes, Features), but we have time.
        # Strategy: We pass the whole window as a feature tensor.
        # The Model (Graph-Liquid-KAN) will handle the temporal dimension internally.
        
        data = Data(
            x = x_window,       # (Time, N, F)
            y = y_window,       # (Time, N)
            mask = mask_window, # (Time, N)
            edge_index = self.graph['edge_index'],
            degree = self.graph['degree'] # For Graphon normalization
        )
        return data

# Validation Test
if __name__ == "__main__":
    ds = SeaLiceGraphDataset("dataset_2023.pt", "spatial_graph.pt")
    sample = ds
    print("Sample X shape:", sample.x.shape) # Expect (30, N, 5)
    print("Sample Edge Index:", sample.edge_index.shape)

```

## Execution Protocol for Phase 2

1. **Run `fetch_lice_history.py**`. *Warning:* This takes hours. Run it in a `screen` or background session. Ensure you have the API credentials from Phase 1.
2. **Run `graph_builder.py**`. This is fast. It generates the static topology.
3. **Run `builder.py**`. This aligns the huge NetCDF with the CSVs. Requires high RAM (>16GB recommended). If you get MemoryError, rewrite the `resample` step to process chunks of `farm_ids`.
4. **Run `dataset.py**`. This verifies everything loads correctly into PyTorch tensors.

